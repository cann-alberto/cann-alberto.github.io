---
---

@ARTICLE{Cannavò20211871,
	author = {Cannavò, Alberto and Calandra, Davide and Pratticò, F. Gabriele and Gatteschi, Valentina and Lamberti, Fabrizio},
	title = {An Evaluation Testbed for Locomotion in Virtual Reality},
	year = {2021},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = {27},
	number = {3},
	pages = {1871 – 1889},
	doi = {10.1109/TVCG.2020.3032440},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100383262&doi=10.1109%2fTVCG.2020.3032440&partnerID=40&md5=815bc3e8c0bed98df32e385fca283398},
	abstract = {A common operation performed in Virtual Reality (VR) environments is locomotion. Although real walking can represent a natural and intuitive way to manage displacements in such environments, its use is generally limited by the size of the area tracked by the VR system (typically, the size of a room) or requires expensive technologies to cover particularly extended settings. A number of approaches have been proposed to enable effective explorations in VR, each characterized by different hardware requirements and costs, and capable to provide different levels of usability and performance. However, the lack of a well-defined methodology for assessing and comparing available approaches makes it difficult to identify, among the various alternatives, the best solutions for selected application domains. To deal with this issue, this article introduces a novel evaluation testbed which, by building on the outcomes of many separate works reported in the literature, aims to support a comprehensive analysis of the considered design space. An experimental protocol for collecting objective and subjective measures is proposed, together with a scoring system able to rank locomotion approaches based on a weighted set of requirements. Testbed usage is illustrated in a use case requesting to select the technique to adopt in a given application scenario. © 2020 IEEE.},
	author_keywords = {evaluation; locomotion; performance; requirements; testbed; user experience; virtual environments; Virtual reality},
	keywords = {Testbeds; Application scenario; Common operations; Comprehensive analysis; Design spaces; Experimental protocols; Objective and subjective measures; Scoring systems; Weighted set; article; human; human experiment; locomotion; outcome assessment; scoring system; virtual reality; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}


@ARTICLE{Cannavo2019125463,
	author = {Cannavo, Alberto and Demartini, Claudio and Morra, Lia and Lamberti, Fabrizio},
	title = {Immersive Virtual Reality-Based Interfaces for Character Animation},
	year = {2019},
	journal = {IEEE Access},
	volume = {7},
	pages = {125463 – 125480},
	doi = {10.1109/ACCESS.2019.2939427},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072557123&doi=10.1109%2fACCESS.2019.2939427&partnerID=40&md5=dae63ca88d1b1a06ddfab7bbac9be106},
	abstract = {Virtual Reality (VR) has increasingly attracted the attention of the computer animation community in search of more intuitive and effective alternatives to the current sophisticated user interfaces. Previous works in the literature already demonstrated the higher affordances offered by VR interaction, as well as the enhanced spatial understanding that arises thanks to the strong sense of immersion guaranteed by virtual environments. These factors have the potential to improve the animators' job, which is tremendously skill-intensive and time-consuming. The present paper explores the opportunities provided by VR-based interfaces for the generation of 3D animations via armature deformation. To the best of the authors' knowledge, for the first time a tool is presented which allows users to manage a complete pipeline supporting the above animation method, by letting them execute key tasks such as rigging, skinning and posing within a well-known animation suite using a customizable interface. Moreover, it is the first work to validate, in both objective and subjective terms, character animation performance in the above tasks and under realistic work conditions involving different user categories. In our experiments, task completion time was reduced by 26%, on average, while maintaining almost the same levels of accuracy and precision for both novice and experienced users. © 2013 IEEE.},
	author_keywords = {3D graphics; Blender; computer animation; evaluation; user interface; virtual reality},
	keywords = {Blending; User interfaces; Virtual reality; 3D graphics; Accuracy and precision; Blender; Character animation; Computer animation; evaluation; Immersive virtual reality; Task completion time; Animation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Gatteschi202253,
	author = {Gatteschi, Valentina and Cannavo, Alberto and Lamberti, Fabrizio and Morra, Lia and Montuschi, Paolo},
	title = {Comparing Algorithms for Aggressive Driving Event Detection Based on Vehicle Motion Data},
	year = {2022},
	journal = {IEEE Transactions on Vehicular Technology},
	volume = {71},
	number = {1},
	pages = {53 – 68},
	doi = {10.1109/TVT.2021.3122197},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118589484&doi=10.1109%2fTVT.2021.3122197&partnerID=40&md5=8688311caeba45a1b8b473ec86b529b9},
	abstract = {Aggressive driving is one of the main causes of fatal crashes. Correctly identifying aggressive driving events still represents a challenge in the literature. Furthermore, datasets available for testing the proposed approaches have some limitations since they generally (a) include only a few types of events, (b) contain data collected with only one device, and (c) are generated in drives that did not fully consider the variety of road characteristics and/or driving conditions. The main objective of this work is to compare the performance of several state-of-The-Art algorithms for aggressive driving event detection (belonging to anomaly detection-, threshold-and machine learning-based categories) on multiple datasets containing sensors data collected with different devices (black-boxes and smartphones), on different vehicles and in different locations. A secondary objective is to verify whether smartphones could replace black-boxes in aggressive/non-Aggressive classification tasks. To this aim, we propose the AD$^2$ (Aggressive Driving Detection) dataset, which contains (i) data collected using multiple devices to evaluate their influence on the algorithm performance, (ii) geographical data useful to analyze the context in which the events occurred, (iii) events recorded in different situations, and (iv) events generated by traveling the same path with aggressive and non-Aggressive driving styles, in order to possibly separate the effects of driving style from those of road characteristics. Our experimental results highlighted the superiority of machine learning-based approaches and underlined the ability of smartphones to ensure a level of performance similar to that of black-boxes.  © 1967-2012 IEEE.},
	author_keywords = {anomaly detection; black-box; classification; Driving behavior; machine learning; smartphone; threshold},
	keywords = {Accidents; Digital storage; Feature extraction; Learning systems; Roads and streets; Time series analysis; Vehicles; Anomaly detection; Black boxes; Driving behaviour; Features extraction; Performances evaluation; Sensor phenomenon and characterizations; Smart phones; Threshold; Time-series analysis; Smartphones},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}



@ARTICLE{Calandra20213147,
	author = {Calandra, Davide and Cannavò, Alberto and Lamberti, Fabrizio},
	title = {Improving AR-powered remote assistance: a new approach aimed to foster operator’s autonomy and optimize the use of skilled resources},
	year = {2021},
	journal = {International Journal of Advanced Manufacturing Technology},
	volume = {114},
	number = {9-10},
	pages = {3147 – 3164},
	doi = {10.1007/s00170-021-06871-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104982913&doi=10.1007%2fs00170-021-06871-4&partnerID=40&md5=2d418def65e6d4569bd3488b0c4232db},
	abstract = {Augmented reality (AR) has a number of applications in industry, but remote assistance represents one of the most prominent and widely studied use cases. Notwithstanding, although the set of functionalities supporting the communication between remote experts and on-site operators grew over time, the way in which remote assistance is delivered has not evolved yet to unleash the full potential of AR technology. The expert typically guides the operator step-by-step, and basically uses AR-based hints to visually support voice instructions. With this approach, skilled human resources may go under-utilized, as the time an expert invests in the assistance corresponds to the time needed by the operator to execute the requested operations. The goal of this work is to introduce a new approach to remote assistance that takes advantage of AR functionalities separately proposed in academic works and commercial products to re-organize the guidance workflow, with the aim to increase the operator’s autonomy and, thus, optimize the use of expert’s time. An AR-powered remote assistance platform able to support the devised approach is also presented. By means of a user study, this approach was compared to traditional step-by-step guidance, with the aim to estimate what is the potential of AR that is still unexploited. Results showed that with the new approach it is possible to reduce the time investment for the expert, allowing the operator to autonomously complete the assigned tasks in a time comparable to step-by-step guidance with a negligible need for further support. © 2021, The Author(s).},
	author_keywords = {Augmented reality; Autonomous operation; Industrial applications; Process efficiency; Remote assistance},
	keywords = {Augmented reality; Academic work; Commercial products; New approaches; Remote assistance; Remote experts; User study; Air navigation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Lamberti20181742,
	abbr={TVCG},
	title = {Virtual Character Animation Based on Affordable Motion Capture and Reconfigurable Tangible Interfaces},
	author = {Lamberti, Fabrizio and Paravati, Gianluca and Gatteschi, Valentina and Cannavo, Alberto and Montuschi, Paolo},
	abstract = {Software for computer animation is generally characterized by a steep learning curve, due to the entanglement of both sophisticated techniques and interaction methods required to control 3D geometries. This paper proposes a tool designed to support computer animation production processes by leveraging the affordances offered by articulated tangible user interfaces and motion capture retargeting solutions. To this aim, orientations of an instrumented prop are recorded together with animator's motion in the 3D space and used to quickly pose characters in the virtual environment. High-level functionalities of the animation software are made accessible via a speech interface, thus letting the user control the animation pipeline via voice commands while focusing on his or her hands and body motion. The proposed solution exploits both off-the-shelf hardware components (like the Lego Mindstorms EV3 bricks and the Microsoft Kinect, used for building the tangible device and tracking animator's skeleton) and free open-source software (like the Blender animation tool), thus representing an interesting solution also for beginners approaching the world of digital animation for the first time. Experimental results in different usage scenarios show the benefits offered by the designed interaction strategy with respect to a mouse & keyboard-based interface both for expert and non-expert users. © 1995-2012 IEEE.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = {24},
	number = {5},
	pages = {1742 – 1755},
	year = {2018},
	doi = {10.1109/TVCG.2017.2690433},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045017032&doi=10.1109%2fTVCG.2017.2690433&partnerID=40&md5=12b0709ef70141b68ff3efd2548b3eee},
	google_scholar_id={2osOgNQ5qMEC},
	author_keywords = {computer animation; human-machine interaction; motion capture; natural user interfaces; Tangible user interfaces},
	keywords = {Blending; Human computer interaction; Open source software; Open systems; User interfaces; Virtual reality; Computer animation; Human machine interaction; Motion capture; Natural user interfaces; Tangible user interfaces; Animation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	selected = {true}

}



@ARTICLE{Cannavò20241003,
	author = {Cannavò, Alberto and Gismondi, Massimo and Lamberti, Fabrizio},
	title = {Virtual prototyping for the textile industry: a framework supporting the production of crocheted objects},
	year = {2024},
	journal = {International Journal of Computer Integrated Manufacturing},
	volume = {37},
	number = {8},
	pages = {1003 – 1024},
	doi = {10.1080/0951192X.2023.2278115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176284386&doi=10.1080%2f0951192X.2023.2278115&partnerID=40&md5=155c9e47040e76670810ace3f5787919},
	abstract = {Over the last years, many progresses have been made in the field of virtual prototyping, pushed by the interest of industries and artisans. Especially in the context of the textile industry, the digitizing of the prototyping stage offers the possibility to validate the product design choices before committing to the market. This paper presents a framework for the virtual prototyping of crocheted objects. The core of the framework is an algorithm that is capable of generating the crocheting patterns for a given object and the corresponding instructions. The instructions are leveraged by the framework to visualize the 3D geometry of the object, and can be also used to craft it. Compared to previous works, the proposed algorithm combines a number of features (primarily, the use of parametric surfaces and the support for short rows) that can reduce the distortions in crafted object shape while also lowering computational cost; the algorithm is also able to consider material- and style-related information. The results of a comparison between the proposed algorithm and state-of-the-art approaches showed improved performance in terms of similarity of the generated shape with the target one, computation time, and appearance of the crafted object. © 2023 Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {crocheting instruction generation algorithm; digital crafting; parametric design; short rows; Virtual prototyping},
	keywords = {E-learning; Electronic commerce; Parameter estimation; Product design; Textile industry; Textiles; Virtual reality; 3D geometry; Computational costs; Crocheting instruction generation algorithm; Digital crafting; Generation algorithm; Instruction generations; Object shape; Parametric design; Parametric surfaces; Short row; Virtual prototyping},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Cannavò20243234,
	author = {Cannavò, Alberto and Stellini, Emanuele and Zhang, Congyi and Lamberti, Fabrizio},
	title = {A Sketch-Based Interface for Facial Animation in Immersive Virtual Reality},
	year = {2024},
	journal = {International Journal of Human-Computer Interaction},
	volume = {40},
	number = {12},
	pages = {3234 – 3252},
	doi = {10.1080/10447318.2023.2185731},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150620231&doi=10.1080%2f10447318.2023.2185731&partnerID=40&md5=25450f251a7e662641d5c9edb672406c},
	abstract = {Creating facial animations using 3D computer graphics represents a very laborious and time-consuming task. Among the numberless approaches for animating faces, the use of blendshapes remains the most common solution because of their simplicity and the ability to produce high-quality results. This approach, however, is also characterized by important drawbacks. With the traditional animation suites, to select the blendshapes to be activated animators are generally requested to memorize the mapping between the blendshapes and the influenced mesh vertices; alternatively, they need to adopt a trial-and-error search within the whole library of available blendshapes. Moreover, the level of expressiveness that can be reached may be lower than expected; this is due to the fact that the possibility to apply transformations to vertices different than just linear translations and mechanisms for adding, e.g., exaggerations, are typically not integrated into the same animation environment. To tackle these issues, this article proposes an immersive virtual reality-based interface that leverages sketches for the direct manipulation of blendshapes. Animators can draw both linear and curved strokes, which are used to automatically extract information about the blendshape to be activated and its weight, the trajectories that associated vertices have to follow, as well as the timing of the overall animation. A user study was carried out with the aim of evaluating the proposed approach on several representative animations tasks. Both objective and subjective measurements were collected. Experimental results showed the benefits of the devised interface in terms of task completion time, animation accuracy, and usability. © 2023 Taylor & Francis Group, LLC.},
	author_keywords = {blendshape; direct manipulation; Facial animation; sketch-based interface; virtual reality},
	keywords = {Drawing (graphics); Linear transformations; Three dimensional computer graphics; Virtual reality; 3D computer graphics; Blendshapes; Direct manipulation; Facial animation; High quality; Immersive virtual reality; Mesh vertex; Sketch based Interface; Time-consuming tasks; Traditional animations; Animation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Cannavo2024,
	author = {Cannavo, Alberto and Offre, Giacomo and Lamberti, Fabrizio},
	title = {A Semi-Automated Pipeline for the Creation of Virtual Fitting Room Experiences Featuring Motion Capture and Cloth Simulation},
	year = {2024},
	journal = {IEEE Computer Graphics and Applications},
	doi = {10.1109/MCG.2024.3521716},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213508918&doi=10.1109%2fMCG.2024.3521716&partnerID=40&md5=2dec97c67628add4a7a96b245167e487},
	abstract = {Technological advancements are prompting the digitization of many industries, including fashion. Many brands are exploring ways to enhance customers' experience, e.g., offering new shopping-oriented services like Virtual Fitting Rooms (VFRs). However, there are still challenges that prevent customers from effectively using these tools for trying on digital garments. Challenges are associated with difficulties in obtaining high-fidelity reconstructions of body shapes and providing realistic visualizations of animated clothes following real-time customers' movements. This paper tackles such lacks by proposing a semi-automated pipeline supporting the creation of VFR experiences by exploiting state-of-the-art techniques for the accurate description and reconstruction of customers' 3D avatars, motion capture-based animation, as well as realistic garment design and simulation. A user study in which the resulting VFR experience was compared with those created with two existing tools showed the benefits of the devised solution in terms of usability, embodiment, model accuracy, perceived value, adoption and purchase intention.  © 1981-2012 IEEE.},
	keywords = {3D modeling; 3D reconstruction; Animation; Fits and tolerances; Garment industry; Hosiery manufacture; Pipe fittings; Body shapes; Cloth simulation; Customer experience; Digital garments; Digitisation; High-fidelity; Motion capture; Real- time; State-of-the-art techniques; Technological advancement; Sales},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus}
}



@CONFERENCE{Cannavo2023318,
	author = {Cannavo, Alberto and Prattico, Filippo Gabriele and Bruno, Alberto and Lamberti, Fabrizio},
	title = {AR-MoCap: Using Augmented Reality to Support Motion Capture Acting},
	year = {2023},
	journal = {Proceedings - 2023 IEEE Conference Virtual Reality and 3D User Interfaces, VR 2023},
	pages = {318 – 327},
	doi = {10.1109/VR55154.2023.00047},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159598302&doi=10.1109%2fVR55154.2023.00047&partnerID=40&md5=e66a0ce75868f5a37578c805e68314af},
	abstract = {Technology is disrupting the way films involving visual effects are produced. Chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are only a few examples of the many changes introduced in the cinema industry over the last years. Although these technologies are getting commonplace, they are presenting new, unexplored challenges to the actors. In particular, when mocap is used to record the actors' movements with the aim of animating digital character models, an increase in the workload can be easily expected for people on stage. In fact, actors have to largely rely on their imagination to understand what the digitally created characters will be actually seeing and feeling. This paper focuses on this specific domain, and aims to demonstrate how Augmented Reality (AR) can be helpful for actors when shooting mocap scenes. To this purpose, we devised a system named AR-MoCap that can be used by actors for rehearsing the scene in AR on the real set before actually shooting it. Through an Optical See-Through Head-Mounted Display (OST-HMD), an actor can see, e.g., the digital characters of other actors wearing mocap suits overlapped in real-time to their bodies. Experimental results showed that, compared to the traditional approach based on physical props and other cues, the devised system can help the actors to position themselves and direct their gaze while shooting the scene, while also improving spatial and social presence, as well as perceived effectiveness.  © 2023 IEEE.},
	author_keywords = {acting rehearsal and performance; augmented reality; body ownership; Collaborative virtual production; Computing methodologies - Computer vision -; Human-centered computing - Applied computing-Arts and humanities - Media arts; Human-centered computing - Applied computing-Arts and humanities - Performing arts; Human-centered computing - Human computer interaction (HCI) - Collaborative interaction -; Human-centered computing - Human computer interaction (HCI) - Mixed / augmented reality -; Image and video acquisition - Motion capture; motion capture; virtual characters; visual effects},
	keywords = {Arts computing; Computer vision; Helmet mounted displays; Human computer interaction; Virtual reality; Acting rehearsal and performance; Applied computing; Body ownership; Collaborative interaction; Collaborative virtual production; Computing methodologies; Computing methodology - computer vision -; Human-centered computing; Human-centered computing - applied computing-art and humanity - medium art; Human-centered computing - applied computing-art and humanity - performing art; Human-centered computing - human computer interaction  - collaborative interaction -; Human-centered computing - human computer interaction  - mixed / augmented reality -; Image and video acquisition - motion capture; Media arts; Motion capture; Performance; Performing arts; Video acquisitions; Virtual character; Virtual production; Visual effects; Augmented reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus}
}




@ARTICLE{Prattico202376,
	author = {Prattico, Filippo Gabriele and Cannavo, Alberto and Calandra, Davide and Lamberti, Fabrizio},
	title = {A Breakdown Study of a Mockup-Based Consumer Haptic Setup for Virtual Reality},
	year = {2023},
	journal = {IEEE Consumer Electronics Magazine},
	volume = {12},
	number = {5},
	pages = {76 – 90},
	doi = {10.1109/MCE.2022.3212571},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139844857&doi=10.1109%2fMCE.2022.3212571&partnerID=40&md5=96f0abd46a3b782eebea87e624adeac8},
	abstract = {Despite the extensive use of visual and audio feedback in virtual reality (VR) experiences, it is possible to acknowledge a still limited exploitation of haptic devices to recreate the sense of touch, especially at the consumer level. In order to support the high variety of haptic stimuli, commercial off-the-shelf devices may need to be used together, combining separate functionalities into a unique solution and compensating for possibly lacking the features of more sophisticated equipment. This article moves from the results of a previous study, which analyzed the impact that two haptic setups built using consumer VR gloves and user-prepared props can have on immersive experiences encompassing an active electromechanical tool (precisely, an electric screwdriver). The said study showed that a combined setup, consisting of a pair of vibrotactile gloves and a custom-made mockup of the screwdriver, could be the most effective from many perspectives. It did not isolate, however, the contribution of each setup component to the users' experience. Thus, the current work operates a breakdown analysis of the reference setup by first identifying a set of simpler, downgraded configurations that could be obtained using the original components, and then evaluating their performance-sophistication tradeoff through a new comparative study. © 2022 IEEE.},
	keywords = {Electromechanical devices; Haptic interfaces; Mockups; Screws; Audio feedbacks; Haptic devices; Haptics; Haptics interfaces; Performances evaluation; Shape; Task analysis; Tracking; Virtual reality experiences; Visual feedback; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}


@ARTICLE{Calandra2024315,
	author = {Calandra, Davide and Pratticò, F. Gabriele and Cannavò, Alberto and Casetti, Claudio and Lamberti, Fabrizio},
	title = {Digital twin- and extended reality-based telepresence for collaborative robot programming in the 6G perspective},
	year = {2024},
	journal = {Digital Communications and Networks},
	volume = {10},
	number = {2},
	pages = {315 – 327},
	doi = {10.1016/j.dcan.2022.10.007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189436662&doi=10.1016%2fj.dcan.2022.10.007&partnerID=40&md5=dd50d18e27181fb2ae5decb4439edf1e},
	abstract = {In the context of Industry 4.0, a paradigm shift from traditional industrial manipulators to Collaborative Robots (CRs) is ongoing, with the latter serving ever more closely humans as auxiliary tools in many production processes. In this scenario, continuous technological advancements offer new opportunities for further innovating robotics and other areas of next-generation industry. For example, 6G could play a prominent role due to its human-centric view of the industrial domains. In particular, its expected dependability features will pave the way for new applications exploiting highly effective Digital Twin (DT)- and eXtended Reality (XR)-based telepresence. In this work, a novel application for the above technologies allowing two distant users to collaborate in the programming of a CR is proposed. The approach encompasses demanding data flows (e.g., point cloud-based streaming of collaborating users and robotic environment), with network latency and bandwidth constraints. Results obtained by analyzing this approach from the viewpoint of network requirements in a setup designed to emulate 6G connectivity indicate that the expected performance of forthcoming mobile networks will make it fully feasible in principle. © 2022 Chongqing University of Posts and Telecommunications},
	author_keywords = {6G; Augmented reality; Collaborative robots; Digital twin; Industry 4.0; Point cloud streaming; Telepresence; Virtual reality},
	keywords = {Augmented reality; Collaborative robots; Industrial manipulators; Industry 4.0; Virtual reality; Visual communication; 6g; Collaborative robots; Human-centric; New applications; Paradigm shifts; Point cloud streaming; Point-clouds; Production process; Technological advancement; Telepresence; Robot programming},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}


@CONFERENCE{Jiang2021,
	author = {Jiang, Ying and Zhang, Congyi and Fu, Hongbo and Cannavo, Alberto and Lamberti, Fabrizio and Lau, Henry Y K and Wang, Wenping},
	title = {Handpainter - 3d sketching in vr with hand-based physical proxy},
	year = {2021},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/3411764.3445302},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106674032&doi=10.1145%2f3411764.3445302&partnerID=40&md5=69dc592893042028c7343e9b14bd2560},
	abstract = {3D sketching in virtual reality (VR) enables users to create 3D virtual objects intuitively and immersively. However, previous studies showed that mid-air drawing may lead to inaccurate sketches. To address this issue, we propose to use one hand as a canvas proxy and the index fnger of the other hand as a 3D pen. To this end, we frst perform a formative study to compare two-handed interaction with tablet-pen interaction for VR sketching. Based on the fndings of this study, we design HandPainter, a VR sketching system which focuses on the direct use of two hands for 3D sketching without requesting any tablet, pen, or VR controller. Our implementation is based on a pair of VR gloves, which provide hand tracking and gesture capture. We devise a set of intuitive gestures to control various functionalities required during 3D sketching, such as canvas panning and drawing positioning. We show the efectiveness of HandPainter by presenting a number of sketching results and discussing the outcomes of a user study-based comparison with mid-air drawing and tablet-based sketching tools. © 2021 ACM.},
	author_keywords = {3d sketching; Hand-based interaction; Vr},
	keywords = {3D sketching; Air drawing; Hand tracking; Pen interactions; Sketching tools; Two-handed interaction; User study; Virtual objects; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Cannavò2024,
	author = {Cannavò, Alberto and Pacchiotti, Simona and Retta, Nicola and Terzoli, Martina and Spallone, Roberta and Lamberti, Fabrizio},
	title = {Passive Haptics and Conversational Avatars for Interacting with Ancient Egypt Remains in High-Fidelity Virtual Reality Experiences},
	year = {2024},
	journal = {Journal on Computing and Cultural Heritage},
	volume = {17},
	number = {2},
	doi = {10.1145/3648003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191814296&doi=10.1145%2f3648003&partnerID=40&md5=5d0ae569dbe1422fed1a64e03cb365f2},
	abstract = {As extended reality continues to grow, new possibilities arise to provide users with novel ways to experience cultural heritage (CH). In particular, applications based on virtual reality (VR), such as virtual museums, have gained increasing popularity, since they can offer new ways for preserving and presenting CH content that are not feasible in physical museums. Despite the numerous benefits, the level of immersion and presence provided by VR experiences still present challenges that could hinder the effectiveness of this technology in the CH context. In this perspective, it is crucial to provide the users with high-fidelity experiences, in which also the interaction with the objects and the characters populating virtual environments are realistic and natural. This article focuses on this challenge and specifically investigates how the combined use of tangible and speech interfaces can help improve the overall experience. To this aim, a immersive VR experience is proposed, which allows the users to manipulate virtual objects belonging to a museum collection (in the specific case, Ancient Egypt remains) by physically operating on 3D printed replicas and to talk with a curator's avatar to get explanations by using their voice. A user study was conducted to evaluate the impact of the considered interfaces on immersion, presence, user experience, usability, and intention to visit, comparing the richest configuration against simpler setups obtained by either removing the tangible interface, the speech interface, or both (and using only handheld controllers). The results show that the combined use of the two interfaces can effectively contribute at making the CH experience in VR more engaging. Copyright © 2024 held by the owner/author(s).},
	author_keywords = {Additional Key Words and PhrasesVirtual museums; Speech User Interfaces; Tangible User Interfaces; Virtual Humans},
	keywords = {3D printing; Three dimensional computer graphics; User interfaces; Additional key word and phrasesvirtual museum; Ancient Egypt; Cultural heritages; High-fidelity; Key words; Speech user interfaces; Tangible interfaces; Tangible user interfaces; Virtual humans; Virtual reality experiences; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}






@ARTICLE{Prattico20211157,
	author = {Prattico, F. Gabriele and Lamberti, Fabrizio and Cannavo, Alberto and Morra, Lia and Montuschi, Paolo},
	title = {Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for Autonomous Vehicle-to-Pedestrian Communication},
	year = {2021},
	journal = {IEEE Transactions on Vehicular Technology},
	volume = {70},
	number = {2},
	pages = {1157 – 1168},
	doi = {10.1109/TVT.2021.3054312},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100451262&doi=10.1109%2fTVT.2021.3054312&partnerID=40&md5=5f30a4091b19a45243a2daad6a957174},
	abstract = {Providing pedestrians and other vulnerable road users with a clear indication about a fully autonomous vehicle status and intentions is crucial to make them coexist. In the last few years, a variety of external interfaces have been proposed, leveraging different paradigms and technologies including vehicle-mounted devices (like LED panels), short-range on-road projections, and road infrastructure interfaces (e.g., special asphalts with embedded displays). These designs were experimented in different settings, using mockups, specially prepared vehicles, or virtual environments, with heterogeneous evaluation metrics. Promising interfaces based on Augmented Reality (AR) have been proposed too, but their usability and effectiveness have not been tested yet. This paper aims to complement such body of literature by presenting a comparison of state-of-the-art interfaces and new designs under common conditions. To this aim, an immersive Virtual Reality-based simulation was developed, recreating a well-known scenario represented by pedestrians crossing in urban environments under non-regulated conditions. A user study was then performed to investigate the various dimensions of vehicle-to-pedestrian interaction leveraging objective and subjective metrics. Even though no interface clearly stood out over all the considered dimensions, one of the AR designs achieved state-of-the-art results in terms of safety and trust, at the cost of higher cognitive effort and lower intuitiveness compared to LED panels showing anthropomorphic features. Together with rankings on the various dimensions, indications about advantages and drawbacks of the various alternatives that emerged from this study could provide important information for next developments in the field.  © 1967-2012 IEEE.},
	author_keywords = {augmented reality; Fully autonomous vehicles; human-machine interaction; pedestrian crossing; vehicle-to-pedestrian communication; virtual reality},
	keywords = {Augmented reality; Display devices; Interface states; Light emitting diodes; Pedestrian safety; Road vehicles; Roads and streets; Virtual reality; Cognitive efforts; Evaluation metrics; Fully-autonomous vehicles; Immersive virtual reality; Information sources; Reality interface; Road infrastructures; Urban environments; Autonomous vehicles},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Cannavò2024,
	author = {Cannavò, Alberto and Bottino, Francesco and Lamberti, Fabrizio},
	title = {Supporting motion-capture acting with collaborative Mixed Reality},
	year = {2024},
	journal = {Computers and Graphics},
	volume = {124},
	doi = {10.1016/j.cag.2024.104090},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204675352&doi=10.1016%2fj.cag.2024.104090&partnerID=40&md5=da75b395f07d9a7661db0fda4f8194f0},
	abstract = {Technologies such as chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are revolutionizing how films featuring visual effects are produced. Despite their popularity, these technologies have introduced new challenges for actors. An increased workload is faced when digital characters are animated via mocap, since actors are requested to use their imagination to envision what characters see and do on set. This work investigates how Mixed Reality (MR) technology can support actors during mocap sessions by presenting a collaborative MR system named CoMR-MoCap, which allows actors to rehearse scenes by overlaying digital contents onto the real set. Using a Video See-Through Head Mounted Display (VST-HMD), actors can see digital representations of performers in mocap suits and digital scene contents in real time. The system supports collaboration, enabling multiple actors to wear both mocap suits to animate digital characters and VST-HMDs to visualize the digital contents. A user study involving 24 participants compared CoMR-MoCap to the traditional method using physical props and visual cues. The results showed that CoMR-MoCap significantly improved actors’ ability to position themselves and direct their gaze, and it offered advantages in terms of usability, spatial and social presence, embodiment, and perceived effectiveness over the traditional method. © 2024},
	author_keywords = {Acting rehearsal and performance; Collaborative virtual production; Mixed reality; Motion capture; Visual effects},
	keywords = {Head-up displays; Helmet mounted displays; Layered manufacturing; Mixed reality; Acting rehearsal and performance; Collaborative virtual production; Digital characters; Digital contents; Mixed reality; Motion capture; Performance; Virtual production; Visual effects; Wall motion; Motion capture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}



@ARTICLE{Cannavò202051,
	author = {Cannavò, Alberto and Zhang, Congyi and Wang, Wenping and Lamberti, Fabrizio},
	title = {Posing 3D Characters in Virtual Reality Through In-the-Air Sketches},
	year = {2020},
	journal = {Communications in Computer and Information Science},
	volume = {1300},
	pages = {51 – 61},
	doi = {10.1007/978-3-030-63426-1_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097641575&doi=10.1007%2f978-3-030-63426-1_6&partnerID=40&md5=a3e1bfab476e5a4d1075b5aa72268f25},
	abstract = {Generating computer animations is a very labor-intensive task, which requires animators to operate with sophisticated interfaces. Hence, researchers continuously experiment with alternative interaction paradigms that could possibly ease the above task. Among others, sketching represents a valid alternative to traditional interfaces since it can make interactions more expressive and intuitive; however, although the literature proposes several solutions leveraging sketch-based interfaces to solve different computer graphics challenges, generally they are not fully integrated in the computer animation pipeline. At the same time, Virtual Reality (VR), is becoming commonplace in many domains, and recently started to be recognized as capable to make it easier also the animators’ job, by improving their spatial understanding of the animated scene and providing them with interfaces characterized by higher usability and effectiveness. Based on all of the above, this paper presents an add-on for a well-known animation suite that combines the advantages offered by a sketch-based interface and VR to let animators define poses and create virtual character animations in an immersive environment. © 2020, Springer Nature Switzerland AG.},
	author_keywords = {3D Animation; Sketch-based interfaces; Virtual reality},
	keywords = {Animation; Arts computing; 3D characters; Computer animation; Fully integrated; Immersive environment; Interaction paradigm; Labor intensive; Sketch based Interface; Virtual character; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus}
}



@ARTICLE{Cannavo20216,
	author = {Cannavo, Alberto and Lamberti, Fabrizio},
	title = {How Blockchain, Virtual Reality, and Augmented Reality are Converging, and Why},
	year = {2021},
	journal = {IEEE Consumer Electronics Magazine},
	volume = {10},
	number = {5},
	pages = {6 – 13},
	doi = {10.1109/MCE.2020.3025753},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091949011&doi=10.1109%2fMCE.2020.3025753&partnerID=40&md5=d1455b63ea56f641b3c1834e78d87ddc},
	abstract = {Nowadays, breakthrough technologies, such as virtual reality (VR), augmented reality (AR), and Blockchain, have definitively attracted the attention of a huge number of investors worldwide. Although, at first glance, Blockchain (traditionally used for financial services) seems to have little to none to share with VR and AR (originally adopted for entertainment), in the last few years several use cases started to appear showing effective ways to integrate these technologies. In this article, an overview of opportunities investigated by current solutions combining VR, AR, and Blockchain will be discussed, showing how they allowed both companies and academic researchers cope with issues affecting traditional services and products in a rather heterogenous set of application domains. Opportunities that could foster the convergence of these technologies and boost them further are also discussed. © 2012 IEEE.},
	keywords = {Augmented reality; Virtual reality; Breakthrough technology; Financial service; Traditional services; Blockchain},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Cannavò2020,
	author = {Cannavò, Alberto and De Pace, Francesco and Salaroglio, Federico and Lamberti, Fabrizio},
	title = {A visual editing tool supporting the production of 3D interactive graphics assets for public exhibitions},
	year = {2020},
	journal = {International Journal of Human Computer Studies},
	volume = {141},
	doi = {10.1016/j.ijhcs.2020.102450},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084080088&doi=10.1016%2fj.ijhcs.2020.102450&partnerID=40&md5=429dd310b84d535f0d3b815765bf95a3},
	abstract = {The introduction of interactive assets in public exhibitions is capable to significantly enhance the visitors’ user experience. However, the creation of interactive applications could represent a challenging task, especially for users lacking computer skills. Visual programming languages (VPLs) – one of the instruments belonging to the broad categories of methods and tools devised to support end-user development (EUD) – promise to offer an intuitive way to overcome these limitations, by providing easy-to-use and efficient interfaces for encoding applications’ logic. Moving from these considerations, this paper first analyses pros and cons of tools devised so far to support the generation of interactive contents. Then, it presents the design of a new tool named Visual Scene Editor (VSE), which allows users with little to no programming skills to create 3D interactive applications by combining available assets through an interactive, visual process. Both objective and subjective measurements have been collected with both skilled and unskilled users to evaluate the performance of the proposed tool. A comparison with existing solutions shows a reduction in the time required to complete the assigned tasks, of the complexity of the logic created, as well as of the number of errors made, confirming the suitability of the VSE for the said purpose. © 2020 Elsevier Ltd},
	author_keywords = {3D Graphics; Augmented reality (AR); End-user development (EUD); Human-machine interaction (HMI); Interactive assets; Natural user interfaces (NUIs); Virtual reality (VR); Visual programming languages (VPLs)},
	keywords = {Computer circuits; Exhibitions; Interactive devices; Three dimensional computer graphics; User experience; 3D interactive graphics; Computer skill; End user development(EUD); Interactive applications; Interactive contents; Objective and subjective measurements; Programming skills; Visual programming languages; Visual languages},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Lamberti20191655,
	author = {Lamberti, Fabrizio and Gatteschi, Valentina and Sanna, Andrea and Cannavò, Alberto},
	title = {A Multimodal Interface for Virtual Character Animation Based on Live Performance and Natural Language Processing},
	year = {2019},
	journal = {International Journal of Human-Computer Interaction},
	volume = {35},
	number = {18},
	pages = {1655 – 1671},
	doi = {10.1080/10447318.2018.1561068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059750274&doi=10.1080%2f10447318.2018.1561068&partnerID=40&md5=5beb8a70a314e10f4e39b193ecc71b90},
	abstract = {Virtual character animation is receiving an ever-growing attention by researchers, who proposed already many tools with the aim to improve the effectiveness of the production process. In particular, significant efforts are devoted to create animation systems suited also to non-skilled users, in order to let them benefit from a powerful communication instrument that can improve information sharing in many contexts like product design, education, marketing, etc. Apart from methods based on the traditional Windows-Icons-Menus-Pointer (WIMP) paradigms, solutions devised so far leverage approaches based on motion capture/retargeting (the so-called performance-based approaches), on non-conventional interfaces (voice inputs, sketches, tangible props, etc.), or on natural language processing (NLP) over text descriptions (e.g., to automatically trigger actions from a library). Each approach has its drawbacks, though. Performance-based methods are difficult to use for creating non-ordinary movements (flips, handstands, etc.); natural interfaces are often used for rough posing, but results need to be later refined; automatic techniques still produce poorly realistic animations. To deal with the above limitations, we propose a multimodal animation system that combines performance- and NLP-based methods. The system recognizes natural commands (gestures, voice inputs) issued by the performer, extracts scene data from a text description and creates live animations in which pre-recorded character actions can be blended with performer’s motion to increase naturalness. © 2018, © 2018 Taylor & Francis Group, LLC.},
	keywords = {Animation; Character recognition; Product design; Automatic technique; Information sharing; Multi-modal interfaces; Multimodal animations; Natural interfaces; Performance based approach; Performance based method; Production process; Natural language processing systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}


@ARTICLE{Calandra2023985,
	author = {Calandra, Davide and De Lorenzis, Federico and Cannavò, Alberto and Lamberti, Fabrizio},
	title = {Immersive virtual reality and passive haptic interfaces to improve procedural learning in a formal training course for first responders},
	year = {2023},
	journal = {Virtual Reality},
	volume = {27},
	number = {2},
	pages = {985 – 1012},
	doi = {10.1007/s10055-022-00704-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139668282&doi=10.1007%2fs10055-022-00704-9&partnerID=40&md5=260c0204242b5ec7d035e9b610a2caca},
	abstract = {One key aspect for the safety and success of first responders’ operations is the compliance, during the intervention, with all the safety procedures and prescribed behaviors. Although real-world simulation exercises are considered as the best way to verify if operators are ready to handle emergency situations, they are not always a viable approach. Firefighting courses, for example, do not usually include this kind of activities, due to the numerous hazards related to deploying controlled fires for the simulation. However, traditional training approaches based on class lessons and multimedia learning material may not be particularly effective for teaching practical skills and procedural behaviors. In this work, the use of a Virtual Reality Training Simulation (VRTS) combined with passive haptic interfaces and a real-time fire simulation logic is investigated as a complement to a traditional video-based training approach used in the context of forest firefighting. The teaching of safety concepts and correct use of individual firefighting tools was selected as a use case, and a user study involving 45 trainees was carried out in the context of an existing training course. One third of the trainees attended the traditional video-based lessons of the course, whereas the remaining ones also took part to a practice training session, half of them with the devised VRTS, the others in the real world. Experimental results showed that the additional use of the devised VRTS improved the trainees’ procedural learning, as well as their motivation and perceived quality of the overall learning experience. © 2022, The Author(s).},
	author_keywords = {Fire simulation; First responders; Forest firefighting; Passive haptics; Video-based training; Virtual reality},
	keywords = {E-learning; Fire extinguishers; Fires; Forestry; Haptic interfaces; Teaching; Fire simulation; First responders; Forest firefighting; Haptics; Haptics interfaces; Passive haptic; Procedural learning; Training simulation; Video-based training; Virtual reality training; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}



@ARTICLE{Lamberti20171989,
	author = {Lamberti, Fabrizio and Paravati, Gianluca and Gatteschi, Valentina and Cannavo, Alberto},
	title = {Supporting web analytics by aggregating user interaction data from heterogeneous devices using viewport-DOM-based heat maps},
	year = {2017},
	journal = {IEEE Transactions on Industrial Informatics},
	volume = {13},
	number = {4},
	pages = {1989 – 1999},
	doi = {10.1109/TII.2017.2658663},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029360675&doi=10.1109%2fTII.2017.2658663&partnerID=40&md5=64bc978845b8dd5269a0f16e854b6be4},
	abstract = {The players of the digital industry look at network Big Data as an incredible source of revenues, which can allow them to design products, services, and market strategies ever more tailored to users' interests and needs. This is the case of data collected by Web analytics tools, which describe the way users interact with Web contents and where their attention focuses onto during navigation. Given the complexity of information to analyze, existing tools often make use of visualization strategies to represent data aggregated throughout separate sessions and multiple users. In particular, heat maps are often adopted to study the distribution of mouse activity and identify page regions that are more frequently reached during interaction. Unfortunately, since Web contents are accessed via ever more heterogeneous devices, region-based heat maps cannot be exploited anymore to aggregate data concerning user's attention, since the same Web content may move to another page location or exhibit a different aspect depending on the access device used or the user agent setup. This paper presents the design of a visual analytics framework capable to deal with the above limitation by adopting a data collection approach that combines information about regions displayed with information about page structure. This way, the well-known heat map-based visualization can be produced, where interactions can be aggregated on a per-element basis independently of the specific access configuration. Experimental results showed that the framework succeeds in accurately quantifying user's attention and replicating results obtained by manual processing. © 2005-2012 IEEE.},
	author_keywords = {Data visualization; heat maps; interaction patterns; network Big Data; user's attention; Web analytics; Web navigation},
	keywords = {Data visualization; Product design; User interfaces; Visualization; Websites; Heat maps; Interaction pattern; user's attention; Web analytics; Web navigation; Big data},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Cannavò20241811,
	author = {Cannavò, Alberto and Castiello, Antonio and Pratticò, F. Gabriele and Mazali, Tatiana and Lamberti, Fabrizio},
	title = {Immersive movies: the effect of point of view on narrative engagement},
	year = {2024},
	journal = {AI and Society},
	volume = {39},
	number = {4},
	pages = {1811 – 1825},
	doi = {10.1007/s00146-022-01622-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146338700&doi=10.1007%2fs00146-022-01622-9&partnerID=40&md5=236ea61e6d53075dc40d2a6f8336d45c},
	abstract = {Cinematic virtual reality (CVR) offers filmmakers a wide range of possibilities to explore new techniques regarding movie scripting, shooting and editing. Despite the many experiments performed so far both with both live action and computer-generated movies, just a few studies focused on analyzing how the various techniques actually affect the viewers’ experience. Like in traditional cinema, a key step for CVR screenwriters and directors is to choose from which perspective the viewers will see the scene, the so-called point of view (POV). The aim of this paper is to understand to what extent watching an immersive movie from a specific POV could impact the narrative engagement (NE), i.e., the viewers’ sensation of being immersed in the movie environment and being connected with its characters and story. Two POVs that are typically used in CVR, i.e., first-person perspective (1-PP) and external perspective (EP), are investigated through a user study in which both objective and subjective metrics were collected. The user study was carried out by leveraging two live action 360° short films with distinct scripts. The results suggest that the 1-PP experience could be more pleasant than the EP one in terms of overall NE and narrative presence, or even for all the NE dimensions if the potential of that POV is specifically exploited. © The Author(s) 2023.},
	author_keywords = {360°; Cinematic VR; External perspective; First-person perspective; Immersive videos; Omnidirectional; User study},
	keywords = {Motion pictures; 360°; Cinematic VR; Cinematics; External perspective; First-person perspectives; Immersive; Immersive video; Live actions; Omnidirectional; User study; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}


@ARTICLE{Cannavò20241763,
	author = {Cannavò, Alberto and De Lorenzis, Federico and Pratticò, Filippo Gabriele and Galante, Lorenzo and Lamberti, Fabrizio},
	title = {On the Quality of the Experience With Virtual Reality-Based Instructional Tools for Science Lab Activities},
	year = {2024},
	journal = {Journal of Educational Computing Research},
	volume = {62},
	number = {7},
	pages = {1763 – 1797},
	doi = {10.1177/07356331241270658},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202772391&doi=10.1177%2f07356331241270658&partnerID=40&md5=a9476d11c50709d37221951b21bc9dcb},
	abstract = {Today, immersive technologies like Virtual Reality (VR) are regarded as disruptive tools in many domains, including education. While the body of literature in the field is growing, studies that present contrasting findings are not uncommon. In fact, although there is evidence of the benefits brought by VR in the educational processes, in some cases the effects of a possible trade-off between learning effectiveness and quality of the learning experience (or QoLE) may be observed. The two dimensions are difficult to disentangle, as besides learning effectiveness, other factors like motivation, technology acceptability, workload, presence, immersion, engagement, and usability come to play. This paper digs into the above scenario by focusing on the QoLE of immersive VR-based learning and comparing it with that of two conventional approaches (a physical prop-based one and a 3D desktop application). Separation of the two dimensions is pursued by imposing equality of the learning performance achieved with the three approaches, aiming at getting rid of possible confounding factors. From the results of the user study performed in the context of a STEM-related laboratory activity, the VR-based approach appeared to be generally superior to the prop-based approach and showed several advantages over the 3D desktop application. © The Author(s) 2024.},
	author_keywords = {laboratory activities; learning experience; learning performance; technology-aided education; virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}



@ARTICLE{Lamberti202036,
	author = {Lamberti, Fabrizio and Cannavo, Alberto and Montuschi, Paolo},
	title = {Is Immersive Virtual Reality the Ultimate Interface for 3D Animators?},
	year = {2020},
	journal = {Computer},
	volume = {53},
	number = {4},
	pages = {36 – 45},
	doi = {10.1109/MC.2019.2908871},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083449865&doi=10.1109%2fMC.2019.2908871&partnerID=40&md5=1617145ba7d49eac43aacacb470b8907},
	abstract = {Creating computer animations is a labor-intensive task. Existing virtual reality (VR)-based animation solutions offer only heterogeneous subsets of traditional tools' functionalities. We present an add-on for the Blender animation suite that enables users to switch between native and immersive VR-based interfaces and employ the latter to perform a representative set of tasks. © 1970-2012 IEEE.},
	keywords = {Animation; Blending; Based animations; Computer animation; Immersive virtual reality; Immersive VR; Labor intensive; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}