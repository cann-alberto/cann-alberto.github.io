Scopus
EXPORT DATE: 02 April 2025

@ARTICLE{Cannavò202369,
	author = {Cannavò, Alberto and Pesando, Roberto and Lamberti, Fabrizio},
	title = {A Framework for Animating Customized Avatars from Monocular Videos in Virtual Try-On Applications},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {14218 LNCS},
	pages = {69 – 88},
	doi = {10.1007/978-3-031-43401-3_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172282603&doi=10.1007%2f978-3-031-43401-3_5&partnerID=40&md5=736a8b0dea4e79d51eece9e6b1d48119},
	abstract = {Generating real-time animations for customized avatars is becoming of paramount importance, especially in Virtual Try-On applications. This technology allows customers to explore or “try on” products virtually. Despite the numerous benefits of this technology, there are some aspects that prevent its applicability in real scenarios. The first limitation regards the difficulties in generating expressive avatar animations. Moreover, potential customers usually expressed concerns regarding the fidelity of the animations. To overcome these two limitations, the current paper is aimed at presenting a framework for animating customized avatars based on state-of-the-art techniques. The focus of the proposed work mainly relies on aspects regarding the animation of the customized avatars. More specifically, the framework encompasses two components. The first one automatizes the operations needed for generating the data structures used for the avatar animation. This component assumes that the mesh of the avatar is described through the Sparse Unified Part-Based Human Representation (SUPR). The second component of the framework is designed to animate the avatar through motion capture by making use of the MediaPipe Holistic pipeline. Experimental evaluations were carried out aimed at assessing the solutions proposed for pose beautification and joint estimations. Results demonstrated improvements in the quality of the reconstructed animation from both an objective and subjective point of view. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Animating customized avatar; MediaPipe holistic pipeline; Pose beautification; SUPR},
	keywords = {Animation; 'current; Animating customized avatar; Mediapipe holistic pipeline; Monocular video; Part based; Pose beautification; Potential customers; Real-time animations; Sparse unified part-based human representation; Virtual try-on; Pipelines},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Cannavo2019125463,
	author = {Cannavo, Alberto and Demartini, Claudio and Morra, Lia and Lamberti, Fabrizio},
	title = {Immersive Virtual Reality-Based Interfaces for Character Animation},
	year = {2019},
	journal = {IEEE Access},
	volume = {7},
	pages = {125463 – 125480},
	doi = {10.1109/ACCESS.2019.2939427},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072557123&doi=10.1109%2fACCESS.2019.2939427&partnerID=40&md5=dae63ca88d1b1a06ddfab7bbac9be106},
	abstract = {Virtual Reality (VR) has increasingly attracted the attention of the computer animation community in search of more intuitive and effective alternatives to the current sophisticated user interfaces. Previous works in the literature already demonstrated the higher affordances offered by VR interaction, as well as the enhanced spatial understanding that arises thanks to the strong sense of immersion guaranteed by virtual environments. These factors have the potential to improve the animators' job, which is tremendously skill-intensive and time-consuming. The present paper explores the opportunities provided by VR-based interfaces for the generation of 3D animations via armature deformation. To the best of the authors' knowledge, for the first time a tool is presented which allows users to manage a complete pipeline supporting the above animation method, by letting them execute key tasks such as rigging, skinning and posing within a well-known animation suite using a customizable interface. Moreover, it is the first work to validate, in both objective and subjective terms, character animation performance in the above tasks and under realistic work conditions involving different user categories. In our experiments, task completion time was reduced by 26%, on average, while maintaining almost the same levels of accuracy and precision for both novice and experienced users. © 2013 IEEE.},
	author_keywords = {3D graphics; Blender; computer animation; evaluation; user interface; virtual reality},
	keywords = {Blending; User interfaces; Virtual reality; 3D graphics; Accuracy and precision; Blender; Character animation; Computer animation; evaluation; Immersive virtual reality; Task completion time; Animation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Gatteschi202253,
	author = {Gatteschi, Valentina and Cannavo, Alberto and Lamberti, Fabrizio and Morra, Lia and Montuschi, Paolo},
	title = {Comparing Algorithms for Aggressive Driving Event Detection Based on Vehicle Motion Data},
	year = {2022},
	journal = {IEEE Transactions on Vehicular Technology},
	volume = {71},
	number = {1},
	pages = {53 – 68},
	doi = {10.1109/TVT.2021.3122197},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118589484&doi=10.1109%2fTVT.2021.3122197&partnerID=40&md5=8688311caeba45a1b8b473ec86b529b9},
	abstract = {Aggressive driving is one of the main causes of fatal crashes. Correctly identifying aggressive driving events still represents a challenge in the literature. Furthermore, datasets available for testing the proposed approaches have some limitations since they generally (a) include only a few types of events, (b) contain data collected with only one device, and (c) are generated in drives that did not fully consider the variety of road characteristics and/or driving conditions. The main objective of this work is to compare the performance of several state-of-The-Art algorithms for aggressive driving event detection (belonging to anomaly detection-, threshold-and machine learning-based categories) on multiple datasets containing sensors data collected with different devices (black-boxes and smartphones), on different vehicles and in different locations. A secondary objective is to verify whether smartphones could replace black-boxes in aggressive/non-Aggressive classification tasks. To this aim, we propose the AD$^2$ (Aggressive Driving Detection) dataset, which contains (i) data collected using multiple devices to evaluate their influence on the algorithm performance, (ii) geographical data useful to analyze the context in which the events occurred, (iii) events recorded in different situations, and (iv) events generated by traveling the same path with aggressive and non-Aggressive driving styles, in order to possibly separate the effects of driving style from those of road characteristics. Our experimental results highlighted the superiority of machine learning-based approaches and underlined the ability of smartphones to ensure a level of performance similar to that of black-boxes.  © 1967-2012 IEEE.},
	author_keywords = {anomaly detection; black-box; classification; Driving behavior; machine learning; smartphone; threshold},
	keywords = {Accidents; Digital storage; Feature extraction; Learning systems; Roads and streets; Time series analysis; Vehicles; Anomaly detection; Black boxes; Driving behaviour; Features extraction; Performances evaluation; Sensor phenomenon and characterizations; Smart phones; Threshold; Time-series analysis; Smartphones},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Prattico2019205,
	author = {Prattico, F. Gabriele and Baldo, Piero and Cannavo, Alberto and Lamberti, Fabrizio},
	title = {Investigating tangible user interaction in mixed-reality robotic games},
	year = {2019},
	journal = {IEEE International Conference on Consumer Electronics - Berlin, ICCE-Berlin},
	volume = {2019-September},
	pages = {205 – 210},
	doi = {10.1109/ICCE-Berlin47944.2019.8966135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078945953&doi=10.1109%2fICCE-Berlin47944.2019.8966135&partnerID=40&md5=c9de5c2195c88f2beeeb024dcf87da06},
	abstract = {Among the emerging trends in Human-Robot Interaction, some of the most frequently used paradigms of interaction involve the use of Tangible User Interfaces. This is especially true also in the field of robotic gaming and, more specifically, in application domains in which commercial off-the-shelf robots and projected Mixed Reality (MR) technology are combined together. The popularity of such interfaces, also in other domains of Human-Machine Interaction, has led to an abundance in the number of gestures that can be used to perform tangible action using these interfaces. However, there are not sufficient pieces of evidence on how these different modalities can impact the user experience, in particular when interacting with a robot in a "phygital play" environment. By moving from this consideration, this paper reports on the efforts that are ongoing with the aim to investigate the impact of diverse gesture sets (which can be performed with the same physical prop) on the perception of interaction with the robotic system. It also presents preliminary insights obtained, which could be exploited to orient further research about the use of such interfaces for interaction in MRbased robotic gaming and related scenarios. © 2019 IEEE.},
	author_keywords = {Game design; Human-Robot Interaction; Mixed Reality; Phygital play; Robotic game; Tangible interfaces; User experience.},
	keywords = {Machine design; Man machine systems; Mixed reality; Robotics; User interfaces; Game design; Human machine interaction; Mixed reality technologies; Phygital play; Tangible interfaces; Tangible user interfaces; User experience; User interaction; Human robot interaction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Cannavò2020113,
	author = {Cannavò, Alberto and D'Alessandro, Arianna and Maglione, Daniele and Marullo, Giorgia and Zhang, Congyi and Lamberti, Fabrizio},
	title = {Automatic generation of affective 3D virtual environments from 2D images},
	year = {2020},
	journal = {VISIGRAPP 2020 - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
	volume = {1},
	pages = {113 – 124},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083563262&partnerID=40&md5=1fab24b343af37d1ecdcfe3819cab4f1},
	abstract = {Today, a wide range of domains encompassing, e.g., movie and video game production, virtual reality simulations, augmented reality applications, make a massive use of 3D computer generated assets. Although many graphics suites already offer a large set of tools and functionalities to manage the creation of such contents, they are usually characterized by a steep learning curve. This aspect could make it difficult for non-expert users to create 3D scenes for, e.g., sharing their ideas or for prototyping purposes. This paper presents a computer-based system that is able to generate a possible reconstruction of a 3D scene depicted in a 2D image, by inferring objects, materials, textures, lights, and camera required for rendering. The integration of the proposed system into a well-known graphics suite enables further refinements of the generated scene using traditional techniques. Moreover, the system allows the users to explore the scene into an immersive virtual environment for better understanding the current objects' layout, and provides the possibility to convey emotions through specific aspects of the generated scene. The paper also reports the results of a user study that was carried out to evaluate the usability of the proposed system from different perspectives. Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
	author_keywords = {Human-computer interaction; Image-based modeling; Scene and object modeling; Virtual reality},
	keywords = {Augmented reality; Computer games; Computer vision; Textures; Virtual reality; 3-D virtual environment; Augmented reality applications; Automatic Generation; Computer-based system; Immersive virtual environments; Steep learning curve; Traditional techniques; Virtual reality simulations; Three dimensional computer graphics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Calandra20213147,
	author = {Calandra, Davide and Cannavò, Alberto and Lamberti, Fabrizio},
	title = {Improving AR-powered remote assistance: a new approach aimed to foster operator’s autonomy and optimize the use of skilled resources},
	year = {2021},
	journal = {International Journal of Advanced Manufacturing Technology},
	volume = {114},
	number = {9-10},
	pages = {3147 – 3164},
	doi = {10.1007/s00170-021-06871-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104982913&doi=10.1007%2fs00170-021-06871-4&partnerID=40&md5=2d418def65e6d4569bd3488b0c4232db},
	abstract = {Augmented reality (AR) has a number of applications in industry, but remote assistance represents one of the most prominent and widely studied use cases. Notwithstanding, although the set of functionalities supporting the communication between remote experts and on-site operators grew over time, the way in which remote assistance is delivered has not evolved yet to unleash the full potential of AR technology. The expert typically guides the operator step-by-step, and basically uses AR-based hints to visually support voice instructions. With this approach, skilled human resources may go under-utilized, as the time an expert invests in the assistance corresponds to the time needed by the operator to execute the requested operations. The goal of this work is to introduce a new approach to remote assistance that takes advantage of AR functionalities separately proposed in academic works and commercial products to re-organize the guidance workflow, with the aim to increase the operator’s autonomy and, thus, optimize the use of expert’s time. An AR-powered remote assistance platform able to support the devised approach is also presented. By means of a user study, this approach was compared to traditional step-by-step guidance, with the aim to estimate what is the potential of AR that is still unexploited. Results showed that with the new approach it is possible to reduce the time investment for the expert, allowing the operator to autonomously complete the assigned tasks in a time comparable to step-by-step guidance with a negligible need for further support. © 2021, The Author(s).},
	author_keywords = {Augmented reality; Autonomous operation; Industrial applications; Process efficiency; Remote assistance},
	keywords = {Augmented reality; Academic work; Commercial products; New approaches; Remote assistance; Remote experts; User study; Air navigation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Lamberti20181742,
	author = {Lamberti, Fabrizio and Paravati, Gianluca and Gatteschi, Valentina and Cannavo, Alberto and Montuschi, Paolo},
	title = {Virtual Character Animation Based on Affordable Motion Capture and Reconfigurable Tangible Interfaces},
	year = {2018},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = {24},
	number = {5},
	pages = {1742 – 1755},
	doi = {10.1109/TVCG.2017.2690433},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045017032&doi=10.1109%2fTVCG.2017.2690433&partnerID=40&md5=12b0709ef70141b68ff3efd2548b3eee},
	abstract = {Software for computer animation is generally characterized by a steep learning curve, due to the entanglement of both sophisticated techniques and interaction methods required to control 3D geometries. This paper proposes a tool designed to support computer animation production processes by leveraging the affordances offered by articulated tangible user interfaces and motion capture retargeting solutions. To this aim, orientations of an instrumented prop are recorded together with animator's motion in the 3D space and used to quickly pose characters in the virtual environment. High-level functionalities of the animation software are made accessible via a speech interface, thus letting the user control the animation pipeline via voice commands while focusing on his or her hands and body motion. The proposed solution exploits both off-the-shelf hardware components (like the Lego Mindstorms EV3 bricks and the Microsoft Kinect, used for building the tangible device and tracking animator's skeleton) and free open-source software (like the Blender animation tool), thus representing an interesting solution also for beginners approaching the world of digital animation for the first time. Experimental results in different usage scenarios show the benefits offered by the designed interaction strategy with respect to a mouse & keyboard-based interface both for expert and non-expert users. © 1995-2012 IEEE.},
	author_keywords = {computer animation; human-machine interaction; motion capture; natural user interfaces; Tangible user interfaces},
	keywords = {Blending; Human computer interaction; Open source software; Open systems; User interfaces; Virtual reality; Computer animation; Human machine interaction; Motion capture; Natural user interfaces; Tangible user interfaces; Animation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32; All Open Access, Green Open Access}
}

@ARTICLE{De Lorenzis202379,
	author = {De Lorenzis, Federico and Visconti, Alessandro and Cannavò, Alberto and Lamberti, Fabrizio},
	title = {MetaLibrary: Towards Social Immersive Environments for Readers},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {14219 LNCS},
	pages = {79 – 87},
	doi = {10.1007/978-3-031-43404-4_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173585228&doi=10.1007%2f978-3-031-43404-4_6&partnerID=40&md5=f2a1f68f7e0196de7fae749a29b96eb3},
	abstract = {The continuous integration of cutting-edge technologies in various fields such as culture and education is leading institutions towards a radical digital evolution. This work studies how one of the key actors of these domains, i.e., libraries, could exploit the digital transformation to reaffirm their position as primary cultural institutions in today’s society. Indeed, there is consensus that, among technologies that could be used to modernize libraries and help them to reach a wider audience, there are Artificial Intelligence (AI) and immersive media like Virtual Reality (VR). In particular, VR has been used to create social platforms that users can join from remote, experiencing virtual environments (VEs) where they can share opinions and perform activities together, thus creating a digital community. In this context, MetaLibrary was created, an immersive VE designed to let readers socialize, attend events with authors, and receive suggestions about books to read from an AI-based recommender system. Deployment is in progress in the city of Turin, Italy. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.},
	author_keywords = {library; metaverse; recommender system; social platform; virtual reality},
	keywords = {Digital libraries; Virtual reality; Continuous integrations; Cultural institutions; Cutting edge technology; Digital evolution; Digital transformation; Immersive environment; Key Actors; Metaverses; Social platform; Work study; Recommender systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Cannavò202277,
	author = {Cannavò, Alberto and Gatteschi, Valentina and Macis, Luca and Lamberti, Fabrizio},
	title = {Automatic Generation of 3D Animations from Text and Images},
	year = {2022},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13445 LNCS},
	pages = {77 – 91},
	doi = {10.1007/978-3-031-15546-8_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137992760&doi=10.1007%2f978-3-031-15546-8_6&partnerID=40&md5=5ee80b344a62bd143f3977449c724185},
	abstract = {The understanding of information in a text description can be improved by visually accompanying it with images or videos. This opportunity is particularly relevant for books and other traditional instructional material. Videos or, more in general, (interactive) graphics contents, can help to increase the effectiveness of this material, by providing, e.g., an animated representation of the steps to be performed to carry out a given procedure. The generation of 3D animated contents, however, is still very labor-intensive and time-consuming. Systems able to speed up this process offering flexible and easy-to-use interfaces are becoming of paramount importance. Hence, this paper describes a system designed to automatically generate a computer graphics video by processing a text description and a set of associated images. The system combines Natural Language Processing and image analysis for extracting information needed to visually represent the procedure depicted in an instruction manual using 3D animations. It relies on a database of 3D models and preconfigured animations that are activated according to the information extracted from the said input. Moreover, by analyzing the images, the system can also generate new animations from scratch. Promising results have been obtained assessing the system performance in a specific use case focused on printers maintenance. © 2022, Springer Nature Switzerland AG.},
	author_keywords = {Computer animation; Image analysis; Natural language processing; Virtual prototyping},
	keywords = {Animation; Image enhancement; Natural language processing systems; 3D animation; Automatic Generation; Computer animation; Graphic contents; Image-analysis; Instructional materials; Interactive graphics; Language processing; Natural language processing; Natural languages; Image analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pratticò201976,
	author = {Pratticò, F. Gabriele and Cannavò, Alberto and Chen, Junchao and Lamberti, Fabrizio},
	title = {User Perception of Robot's Role in Floor Projection-based Mixed-Reality Robotic Games},
	year = {2019},
	journal = {2019 IEEE 23rd International Symposium on Consumer Technologies, ISCT 2019},
	pages = {76 – 81},
	doi = {10.1109/ISCE.2019.8901037},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075629801&doi=10.1109%2fISCE.2019.8901037&partnerID=40&md5=0a19cf5b2966e4ea43d22f3b03c5e3f4},
	abstract = {Within the emerging research area represented by robotic gaming and, specifically, in application domains in which the recent literature suggests to combine commercial off-the-shelf (COTS) robots and projected mixed reality (MR) technology in order to develop engaging games, one of the crucial issues to consider in the design process is how to make the player perceive the robot as having a key role, i.e., to valorize its presence from the user experience point of view. By moving from this consideration, this paper reports efforts that are being carried out with the aim to investigate the impact of diverse game design choices in the above perspective, while at the same time extracting preliminary insights that can be exploited to orient further research in the field of MR-based robotic gaming and related scenarios. © 2019 IEEE.},
	author_keywords = {game design; Human-robot interaction; phygital play; projection-based mixed reality; robotics; user experience},
	keywords = {Commercial off-the-shelf; Human robot interaction; Machine design; Robotics; Design process; Floor projections; Game design; Mixed reality technologies; phygital play; User experience; User perceptions; Mixed reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Cannavò202394,
	author = {Cannavò, Alberto and Kapralos, Bill and Seinfeld, Sofia and Pratticò, Filippo Gabriele and Zhang, Congyi},
	title = {IEEE VR 2023 Workshops: Workshop: 3D Reconstruction, Digital Twinning, and Simulation for Virtual Experiences (ReDigiTS 2023)},
	year = {2023},
	journal = {Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023},
	pages = {94 – 95},
	doi = {10.1109/VRW58643.2023.00024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159714188&doi=10.1109%2fVRW58643.2023.00024&partnerID=40&md5=d89bad0e4120f805d50f3556ba23ab43},
	abstract = {Today, Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) technologies are becoming fundamental across a wide variety of application domains. All these technologies fall under the Extended Reality (XR) spectrum. Areas in which XR technologies and, in particular, immersive experiences have been successfully introduced are expanding daily, from traditional areas, such as entertainment and video game production, to novel scenarios, such as industry, healthcare, smart cities, and autonomous vehicles, to mention a few. The spread of XR and VR is also driven by technological advancements that are making new ways of operating within virtual experiences easier, and sometimes even possible. In particular, the availability of new hardware and software tools make the use of '3D reconstructions', 'digital twins' and 'simulations' commonplace in many XR and VR applications. For example, the possibility to leverage networks with higher bandwidth than never before and lower latency makes it possible to transmit the amount of data needed for massive, real-time digital twins, whereas high-performance hardware enables the reconstruction of high-fidelity digital objects and environments, while lowering the time needed for high-demanding simulations. The aim of the 3D Reconstruction, Digital Twinning, and Simulation for Virtual Experiences (ReDigiTS) workshop is to bring together researchers, practitioners, educators, and students to share ideas and promote further work in this growing area of 3D reconstruction, digital twinning, and simulation in VR and related technologies.  © 2023 IEEE.},
	keywords = {Application programs; Computer aided software engineering; Engineering education; Image reconstruction; Mixed reality; 3D reconstruction; Applications domains; Autonomous Vehicles; Hardware and software; Hardware tools; Immersive; Mixed reality technologies; Spectra's; Technological advancement; Video-games; Augmented reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Cannavò20241003,
	author = {Cannavò, Alberto and Gismondi, Massimo and Lamberti, Fabrizio},
	title = {Virtual prototyping for the textile industry: a framework supporting the production of crocheted objects},
	year = {2024},
	journal = {International Journal of Computer Integrated Manufacturing},
	volume = {37},
	number = {8},
	pages = {1003 – 1024},
	doi = {10.1080/0951192X.2023.2278115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176284386&doi=10.1080%2f0951192X.2023.2278115&partnerID=40&md5=155c9e47040e76670810ace3f5787919},
	abstract = {Over the last years, many progresses have been made in the field of virtual prototyping, pushed by the interest of industries and artisans. Especially in the context of the textile industry, the digitizing of the prototyping stage offers the possibility to validate the product design choices before committing to the market. This paper presents a framework for the virtual prototyping of crocheted objects. The core of the framework is an algorithm that is capable of generating the crocheting patterns for a given object and the corresponding instructions. The instructions are leveraged by the framework to visualize the 3D geometry of the object, and can be also used to craft it. Compared to previous works, the proposed algorithm combines a number of features (primarily, the use of parametric surfaces and the support for short rows) that can reduce the distortions in crafted object shape while also lowering computational cost; the algorithm is also able to consider material- and style-related information. The results of a comparison between the proposed algorithm and state-of-the-art approaches showed improved performance in terms of similarity of the generated shape with the target one, computation time, and appearance of the crafted object. © 2023 Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {crocheting instruction generation algorithm; digital crafting; parametric design; short rows; Virtual prototyping},
	keywords = {E-learning; Electronic commerce; Parameter estimation; Product design; Textile industry; Textiles; Virtual reality; 3D geometry; Computational costs; Crocheting instruction generation algorithm; Digital crafting; Generation algorithm; Instruction generations; Object shape; Parametric design; Parametric surfaces; Short row; Virtual prototyping},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cannavò20243234,
	author = {Cannavò, Alberto and Stellini, Emanuele and Zhang, Congyi and Lamberti, Fabrizio},
	title = {A Sketch-Based Interface for Facial Animation in Immersive Virtual Reality},
	year = {2024},
	journal = {International Journal of Human-Computer Interaction},
	volume = {40},
	number = {12},
	pages = {3234 – 3252},
	doi = {10.1080/10447318.2023.2185731},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150620231&doi=10.1080%2f10447318.2023.2185731&partnerID=40&md5=25450f251a7e662641d5c9edb672406c},
	abstract = {Creating facial animations using 3D computer graphics represents a very laborious and time-consuming task. Among the numberless approaches for animating faces, the use of blendshapes remains the most common solution because of their simplicity and the ability to produce high-quality results. This approach, however, is also characterized by important drawbacks. With the traditional animation suites, to select the blendshapes to be activated animators are generally requested to memorize the mapping between the blendshapes and the influenced mesh vertices; alternatively, they need to adopt a trial-and-error search within the whole library of available blendshapes. Moreover, the level of expressiveness that can be reached may be lower than expected; this is due to the fact that the possibility to apply transformations to vertices different than just linear translations and mechanisms for adding, e.g., exaggerations, are typically not integrated into the same animation environment. To tackle these issues, this article proposes an immersive virtual reality-based interface that leverages sketches for the direct manipulation of blendshapes. Animators can draw both linear and curved strokes, which are used to automatically extract information about the blendshape to be activated and its weight, the trajectories that associated vertices have to follow, as well as the timing of the overall animation. A user study was carried out with the aim of evaluating the proposed approach on several representative animations tasks. Both objective and subjective measurements were collected. Experimental results showed the benefits of the devised interface in terms of task completion time, animation accuracy, and usability. © 2023 Taylor & Francis Group, LLC.},
	author_keywords = {blendshape; direct manipulation; Facial animation; sketch-based interface; virtual reality},
	keywords = {Drawing (graphics); Linear transformations; Three dimensional computer graphics; Virtual reality; 3D computer graphics; Blendshapes; Direct manipulation; Facial animation; High quality; Immersive virtual reality; Mesh vertex; Sketch based Interface; Time-consuming tasks; Traditional animations; Animation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Cannavo2024,
	author = {Cannavo, Alberto and Offre, Giacomo and Lamberti, Fabrizio},
	title = {A Semi-Automated Pipeline for the Creation of Virtual Fitting Room Experiences Featuring Motion Capture and Cloth Simulation},
	year = {2024},
	journal = {IEEE Computer Graphics and Applications},
	doi = {10.1109/MCG.2024.3521716},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213508918&doi=10.1109%2fMCG.2024.3521716&partnerID=40&md5=2dec97c67628add4a7a96b245167e487},
	abstract = {Technological advancements are prompting the digitization of many industries, including fashion. Many brands are exploring ways to enhance customers' experience, e.g., offering new shopping-oriented services like Virtual Fitting Rooms (VFRs). However, there are still challenges that prevent customers from effectively using these tools for trying on digital garments. Challenges are associated with difficulties in obtaining high-fidelity reconstructions of body shapes and providing realistic visualizations of animated clothes following real-time customers' movements. This paper tackles such lacks by proposing a semi-automated pipeline supporting the creation of VFR experiences by exploiting state-of-the-art techniques for the accurate description and reconstruction of customers' 3D avatars, motion capture-based animation, as well as realistic garment design and simulation. A user study in which the resulting VFR experience was compared with those created with two existing tools showed the benefits of the devised solution in terms of usability, embodiment, model accuracy, perceived value, adoption and purchase intention.  © 1981-2012 IEEE.},
	keywords = {3D modeling; 3D reconstruction; Animation; Fits and tolerances; Garment industry; Hosiery manufacture; Pipe fittings; Body shapes; Cloth simulation; Customer experience; Digital garments; Digitisation; High-fidelity; Motion capture; Real- time; State-of-the-art techniques; Technological advancement; Sales},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Cannavò201826,
	author = {Cannavò, Alberto and Ministeri, Giuseppe and Lamberti, Fabrizio and Pratticò, Filippo Gabriele},
	title = {A movement analysis system based on immersive virtual reality and wearable technology for sport training},
	year = {2018},
	journal = {ACM International Conference Proceeding Series},
	pages = {26 – 31},
	doi = {10.1145/3198910.3198917},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055497681&doi=10.1145%2f3198910.3198917&partnerID=40&md5=c97f826c105976ac5153846869c8d482},
	abstract = {The use of virtual reality (VR) is widespread in a growing number of application domains. Continuous technological advancements in the field of computer graphics made VR an interesting tool for learning purposes, especially in sport. Examples can be found in different sports such as rugby, baseball, soccer, golf, etc. This paper presents a VR-based training system that can be used as a self-learning tool to improve the execution of a given technical gesture. In particular, the basketball free throw gesture is considered. To assess the usefulness of the proposed system, experimental tests were carried out in a small-scale setup by involving 18 non-skilled volunteers. Results demonstrated that the designed system can improve the execution of the considered gesture in terms of both timing and spatial positioning compared to an alternative technique based on video projection. © 2018 Association for Computing Machinery.},
	author_keywords = {Human-machine interaction; Sport training; Virtual reality},
	keywords = {E-learning; Sports; Wearable technology; Experimental test; Human machine interaction; Immersive virtual reality; Movement analysis; Spatial positioning; Technological advancement; Training Systems; Video projections; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30}
}

@CONFERENCE{Cannavo2023318,
	author = {Cannavo, Alberto and Prattico, Filippo Gabriele and Bruno, Alberto and Lamberti, Fabrizio},
	title = {AR-MoCap: Using Augmented Reality to Support Motion Capture Acting},
	year = {2023},
	journal = {Proceedings - 2023 IEEE Conference Virtual Reality and 3D User Interfaces, VR 2023},
	pages = {318 – 327},
	doi = {10.1109/VR55154.2023.00047},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159598302&doi=10.1109%2fVR55154.2023.00047&partnerID=40&md5=e66a0ce75868f5a37578c805e68314af},
	abstract = {Technology is disrupting the way films involving visual effects are produced. Chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are only a few examples of the many changes introduced in the cinema industry over the last years. Although these technologies are getting commonplace, they are presenting new, unexplored challenges to the actors. In particular, when mocap is used to record the actors' movements with the aim of animating digital character models, an increase in the workload can be easily expected for people on stage. In fact, actors have to largely rely on their imagination to understand what the digitally created characters will be actually seeing and feeling. This paper focuses on this specific domain, and aims to demonstrate how Augmented Reality (AR) can be helpful for actors when shooting mocap scenes. To this purpose, we devised a system named AR-MoCap that can be used by actors for rehearsing the scene in AR on the real set before actually shooting it. Through an Optical See-Through Head-Mounted Display (OST-HMD), an actor can see, e.g., the digital characters of other actors wearing mocap suits overlapped in real-time to their bodies. Experimental results showed that, compared to the traditional approach based on physical props and other cues, the devised system can help the actors to position themselves and direct their gaze while shooting the scene, while also improving spatial and social presence, as well as perceived effectiveness.  © 2023 IEEE.},
	author_keywords = {acting rehearsal and performance; augmented reality; body ownership; Collaborative virtual production; Computing methodologies - Computer vision -; Human-centered computing - Applied computing-Arts and humanities - Media arts; Human-centered computing - Applied computing-Arts and humanities - Performing arts; Human-centered computing - Human computer interaction (HCI) - Collaborative interaction -; Human-centered computing - Human computer interaction (HCI) - Mixed / augmented reality -; Image and video acquisition - Motion capture; motion capture; virtual characters; visual effects},
	keywords = {Arts computing; Computer vision; Helmet mounted displays; Human computer interaction; Virtual reality; Acting rehearsal and performance; Applied computing; Body ownership; Collaborative interaction; Collaborative virtual production; Computing methodologies; Computing methodology - computer vision -; Human-centered computing; Human-centered computing - applied computing-art and humanity - medium art; Human-centered computing - applied computing-art and humanity - performing art; Human-centered computing - human computer interaction  - collaborative interaction -; Human-centered computing - human computer interaction  - mixed / augmented reality -; Image and video acquisition - motion capture; Media arts; Motion capture; Performance; Performing arts; Video acquisitions; Virtual character; Virtual production; Visual effects; Augmented reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Hogue2023,
	author = {Hogue, Andrew and Cannavo, Alberto},
	title = {Executive Conference Chair Welcome},
	year = {2023},
	journal = {2023 IEEE Gaming, Entertainment, and Media Conference, GEM 2023},
	doi = {10.1109/GEM59776.2023.10389949},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184819448&doi=10.1109%2fGEM59776.2023.10389949&partnerID=40&md5=271539993d1b724e9cac8f521a753545},
	type = {Editorial},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Bronze Open Access}
}

@CONFERENCE{Botto2020299,
	author = {Botto, Carola and Cannavo, Alberto and Cappuccio, Daniele and Morat, Giada and Sarvestani, Amir Nematollahi and Ricci, Paolo and Demarchi, Valentina and Saturnino, Alessandra},
	title = {Augmented Reality for the Manufacturing Industry: The Case of an Assembly Assistant},
	year = {2020},
	journal = {Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020},
	pages = {299 – 304},
	doi = {10.1109/VRW50115.2020.00068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085363137&doi=10.1109%2fVRW50115.2020.00068&partnerID=40&md5=db436f4bdd4fe3c2c6bb84bfadc63bd8},
	abstract = {This paper studies the impact of augmented reality (AR) on manual assembly operations in the manufacturing industry. A review of ARbased solutions in this field shows that assembly assistants capable to support the user in key activities (identification, handling, alignment, joining, adjustment and inspection) are particularly beneficial. Since assembly activities are generally not addressed within comprehensive solutions, an AR-based tool supporting all of them is proposed, and its effectiveness in terms of completion time and error rate is compared with the use of the corresponding paper-based instructions. Results of a user study unveil that the proposed tool generally reduces the number of errors, but the time needed to complete the assembly tends to increase. Limitations of the current solution and potential directions for future work are discussed. © 2020 IEEE.},
	author_keywords = {Applied computing; Computer-aided manufacturing; Human computer interaction (HCI); Interaction paradigms; Mixed/augmented reality},
	keywords = {Augmented reality; Manufacture; Virtual reality; Completion time; Error rate; Manual assembly; Manufacturing industries; Tool supporting; User study; User interfaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@ARTICLE{Prattico202376,
	author = {Prattico, Filippo Gabriele and Cannavo, Alberto and Calandra, Davide and Lamberti, Fabrizio},
	title = {A Breakdown Study of a Mockup-Based Consumer Haptic Setup for Virtual Reality},
	year = {2023},
	journal = {IEEE Consumer Electronics Magazine},
	volume = {12},
	number = {5},
	pages = {76 – 90},
	doi = {10.1109/MCE.2022.3212571},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139844857&doi=10.1109%2fMCE.2022.3212571&partnerID=40&md5=96f0abd46a3b782eebea87e624adeac8},
	abstract = {Despite the extensive use of visual and audio feedback in virtual reality (VR) experiences, it is possible to acknowledge a still limited exploitation of haptic devices to recreate the sense of touch, especially at the consumer level. In order to support the high variety of haptic stimuli, commercial off-the-shelf devices may need to be used together, combining separate functionalities into a unique solution and compensating for possibly lacking the features of more sophisticated equipment. This article moves from the results of a previous study, which analyzed the impact that two haptic setups built using consumer VR gloves and user-prepared props can have on immersive experiences encompassing an active electromechanical tool (precisely, an electric screwdriver). The said study showed that a combined setup, consisting of a pair of vibrotactile gloves and a custom-made mockup of the screwdriver, could be the most effective from many perspectives. It did not isolate, however, the contribution of each setup component to the users' experience. Thus, the current work operates a breakdown analysis of the reference setup by first identifying a set of simpler, downgraded configurations that could be obtained using the original components, and then evaluating their performance-sophistication tradeoff through a new comparative study. © 2022 IEEE.},
	keywords = {Electromechanical devices; Haptic interfaces; Mockups; Screws; Audio feedbacks; Haptic devices; Haptics; Haptics interfaces; Performances evaluation; Shape; Task analysis; Tracking; Virtual reality experiences; Visual feedback; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Cancedda2017447,
	author = {Cancedda, Laura and Cannavò, Alberto and Garofalo, Giuseppe and Lamberti, Fabrizio and Montuschi, Paolo and Paravati, Gianluca},
	title = {Mixed reality-based user interaction feedback for a hand-controlled interface targeted to robot teleoperation},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10325 LNCS},
	pages = {447 – 463},
	doi = {10.1007/978-3-319-60928-7_38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021227753&doi=10.1007%2f978-3-319-60928-7_38&partnerID=40&md5=205f9ebf6bebe701fcfa9b8873f5a131},
	abstract = {The continuous progress in the field of robotics and the diffusion of its related application scenarios in today’s modern world makes human interaction and communication with robots an aspect of fundamental importance. The development of interfaces based on natural interaction paradigms is getting an increasingly captivating topic in Human-Robot Interaction (HRI), due to their intrinsic capabilities in providing ever more intuitive and effective control modalities. Teleoperation systems require to handle a non-negligible amount of information coming from on-board sensors as well as input devices, thus increasing the workload of remote users. This paper presents the design of a 3D User Interface (3DUI) for the control of teleoperated robotic platforms aimed at increasing the interaction efficiency. A hand gesture driven controller is used as input modality to naturally map the position and gestures of the user’s hand to suitable commands for controlling the platform components. The designed interface leverages on mixed reality to provide a visual feedback to the control commands issued by the user. The visualization of the 3DUI is superimposed to the video stream provided by an on-board camera. A user study confirmed that the proposed solution is able to improve the interaction efficiency by significantly reducing the completion time for tasks assigned in a remote reach-and-pick scenario. © Springer International Publishing AG 2017.},
	author_keywords = {3D user interface; Hand-based control; Human-robot interaction; Mixed reality; Robot teleoperation; Visual feedback},
	keywords = {Augmented reality; Efficiency; Feedback; Man machine systems; Mixed reality; Multi agent systems; Remote control; Robotics; User interfaces; Visual communication; Visual servoing; 3D user interface; Amount of information; Human robot Interaction (HRI); Interaction efficiency; Natural interactions; Robot teleoperation; Teleoperation systems; Visual feedback; Human robot interaction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Calandra2024315,
	author = {Calandra, Davide and Pratticò, F. Gabriele and Cannavò, Alberto and Casetti, Claudio and Lamberti, Fabrizio},
	title = {Digital twin- and extended reality-based telepresence for collaborative robot programming in the 6G perspective},
	year = {2024},
	journal = {Digital Communications and Networks},
	volume = {10},
	number = {2},
	pages = {315 – 327},
	doi = {10.1016/j.dcan.2022.10.007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189436662&doi=10.1016%2fj.dcan.2022.10.007&partnerID=40&md5=dd50d18e27181fb2ae5decb4439edf1e},
	abstract = {In the context of Industry 4.0, a paradigm shift from traditional industrial manipulators to Collaborative Robots (CRs) is ongoing, with the latter serving ever more closely humans as auxiliary tools in many production processes. In this scenario, continuous technological advancements offer new opportunities for further innovating robotics and other areas of next-generation industry. For example, 6G could play a prominent role due to its human-centric view of the industrial domains. In particular, its expected dependability features will pave the way for new applications exploiting highly effective Digital Twin (DT)- and eXtended Reality (XR)-based telepresence. In this work, a novel application for the above technologies allowing two distant users to collaborate in the programming of a CR is proposed. The approach encompasses demanding data flows (e.g., point cloud-based streaming of collaborating users and robotic environment), with network latency and bandwidth constraints. Results obtained by analyzing this approach from the viewpoint of network requirements in a setup designed to emulate 6G connectivity indicate that the expected performance of forthcoming mobile networks will make it fully feasible in principle. © 2022 Chongqing University of Posts and Telecommunications},
	author_keywords = {6G; Augmented reality; Collaborative robots; Digital twin; Industry 4.0; Point cloud streaming; Telepresence; Virtual reality},
	keywords = {Augmented reality; Collaborative robots; Industrial manipulators; Industry 4.0; Virtual reality; Visual communication; 6g; Collaborative robots; Human-centric; New applications; Paradigm shifts; Point cloud streaming; Point-clouds; Production process; Technological advancement; Telepresence; Robot programming},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Gold Open Access}
}

@CONFERENCE{Lamberti2019,
	author = {Lamberti, Fabrizio and Cannavo, Alberto and Pirone, Paolo},
	title = {Designing Interactive Robotic Games based on Mixed Reality Technology},
	year = {2019},
	journal = {2019 IEEE International Conference on Consumer Electronics, ICCE 2019},
	doi = {10.1109/ICCE.2019.8661911},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063770228&doi=10.1109%2fICCE.2019.8661911&partnerID=40&md5=06d667749249bb32e8b010ba3ed64ad7},
	abstract = {This paper focuses on an emerging research area represented by robotic gaming and aims to explore the design space of interactive games that combine commercial-off-the-shelf robots and mixed reality. To this purpose, a software platform is developed which allows players to interact with both physical elements and virtual content projected on the ground. A game is then created to show designers how to maximize opportunities offered by such a technology and to build playful experiences. © 2019 IEEE.},
	keywords = {Machine design; Robotics; Design spaces; Interactive games; Interactive robotics; Mixed reality technologies; Physical elements; Software platforms; Mixed reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@CONFERENCE{Ferri2022916,
	author = {Ferri, Riccardo and Cannavo, Alberto and Gabriele Prattico, F. and Lamberti, Fabrizio},
	title = {SPE Selection Technique: A Projection-based Approach for Precise Object Interaction in Immersive Virtual Environments},
	year = {2022},
	journal = {Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2022},
	pages = {916 – 917},
	doi = {10.1109/VRW55335.2022.00309},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129667737&doi=10.1109%2fVRW55335.2022.00309&partnerID=40&md5=15f4f677c29e78bcefc1138686fcf802},
	abstract = {Selecting objects has always represented one of the fundamental tasks required by the majority of immersive applications. Despite the key role played by selection techniques in the design of im-mersive experiences, there is not a dominant or a one-size-fits-all approach yet. In this scenario, the current paper proposes a novel selection technique, named Selection among Projected Elements, which focuses on the selection of distant, small and possibly close, objects. The devised approach was assessed in the context of a virtual heritage application that was specifically designed with the aim of stressing the capabilities of the SPE technique. © 2022 IEEE.},
	keywords = {'current; Immersive application; Immersive virtual environments; Object interactions; Selection techniques; Virtual heritage; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lamberti2023237,
	author = {Lamberti, Fabrizio and Cannavò, Alberto and He, Zaixing and Pepa, Lucia and Yu, Chia-Mu},
	title = {Guest Editorial of the Special Section on Consumer Electronics With Artificial Intelligence},
	year = {2023},
	journal = {IEEE Transactions on Consumer Electronics},
	volume = {69},
	number = {3},
	pages = {237 – 239},
	doi = {10.1109/TCE.2023.3292165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168728725&doi=10.1109%2fTCE.2023.3292165&partnerID=40&md5=78ed3af8f4499be517572916cc61cfd2},
	type = {Editorial},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@CONFERENCE{Jiang2021,
	author = {Jiang, Ying and Zhang, Congyi and Fu, Hongbo and Cannavo, Alberto and Lamberti, Fabrizio and Lau, Henry Y K and Wang, Wenping},
	title = {Handpainter - 3d sketching in vr with hand-based physical proxy},
	year = {2021},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/3411764.3445302},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106674032&doi=10.1145%2f3411764.3445302&partnerID=40&md5=69dc592893042028c7343e9b14bd2560},
	abstract = {3D sketching in virtual reality (VR) enables users to create 3D virtual objects intuitively and immersively. However, previous studies showed that mid-air drawing may lead to inaccurate sketches. To address this issue, we propose to use one hand as a canvas proxy and the index fnger of the other hand as a 3D pen. To this end, we frst perform a formative study to compare two-handed interaction with tablet-pen interaction for VR sketching. Based on the fndings of this study, we design HandPainter, a VR sketching system which focuses on the direct use of two hands for 3D sketching without requesting any tablet, pen, or VR controller. Our implementation is based on a pair of VR gloves, which provide hand tracking and gesture capture. We devise a set of intuitive gestures to control various functionalities required during 3D sketching, such as canvas panning and drawing positioning. We show the efectiveness of HandPainter by presenting a number of sketching results and discussing the outcomes of a user study-based comparison with mid-air drawing and tablet-based sketching tools. © 2021 ACM.},
	author_keywords = {3d sketching; Hand-based interaction; Vr},
	keywords = {3D sketching; Air drawing; Hand tracking; Pen interactions; Sketching tools; Two-handed interaction; User study; Virtual objects; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32}
}

@ARTICLE{Cannavò2024,
	author = {Cannavò, Alberto and Pacchiotti, Simona and Retta, Nicola and Terzoli, Martina and Spallone, Roberta and Lamberti, Fabrizio},
	title = {Passive Haptics and Conversational Avatars for Interacting with Ancient Egypt Remains in High-Fidelity Virtual Reality Experiences},
	year = {2024},
	journal = {Journal on Computing and Cultural Heritage},
	volume = {17},
	number = {2},
	doi = {10.1145/3648003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191814296&doi=10.1145%2f3648003&partnerID=40&md5=5d0ae569dbe1422fed1a64e03cb365f2},
	abstract = {As extended reality continues to grow, new possibilities arise to provide users with novel ways to experience cultural heritage (CH). In particular, applications based on virtual reality (VR), such as virtual museums, have gained increasing popularity, since they can offer new ways for preserving and presenting CH content that are not feasible in physical museums. Despite the numerous benefits, the level of immersion and presence provided by VR experiences still present challenges that could hinder the effectiveness of this technology in the CH context. In this perspective, it is crucial to provide the users with high-fidelity experiences, in which also the interaction with the objects and the characters populating virtual environments are realistic and natural. This article focuses on this challenge and specifically investigates how the combined use of tangible and speech interfaces can help improve the overall experience. To this aim, a immersive VR experience is proposed, which allows the users to manipulate virtual objects belonging to a museum collection (in the specific case, Ancient Egypt remains) by physically operating on 3D printed replicas and to talk with a curator's avatar to get explanations by using their voice. A user study was conducted to evaluate the impact of the considered interfaces on immersion, presence, user experience, usability, and intention to visit, comparing the richest configuration against simpler setups obtained by either removing the tangible interface, the speech interface, or both (and using only handheld controllers). The results show that the combined use of the two interfaces can effectively contribute at making the CH experience in VR more engaging. Copyright © 2024 held by the owner/author(s).},
	author_keywords = {Additional Key Words and PhrasesVirtual museums; Speech User Interfaces; Tangible User Interfaces; Virtual Humans},
	keywords = {3D printing; Three dimensional computer graphics; User interfaces; Additional key word and phrasesvirtual museum; Ancient Egypt; Cultural heritages; High-fidelity; Key words; Speech user interfaces; Tangible interfaces; Tangible user interfaces; Virtual humans; Virtual reality experiences; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Lamberti20181017,
	author = {Lamberti, Fabrizio and Cannavo, Alberto and Pirone, Paolo and Montuschi, Carla and Albera, Roberto},
	title = {Mobile Robot-based Exergames for Navigation Training and Vestibular Rehabilitation},
	year = {2018},
	journal = {2018 9th IEEE Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2018},
	pages = {1017 – 1024},
	doi = {10.1109/UEMCON.2018.8796712},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063801172&doi=10.1109%2fUEMCON.2018.8796712&partnerID=40&md5=b2066715eb7e09c51b14f431b4ea3dd2},
	abstract = {The vestibular system is the leading sensory system that contributes to the sense of balance and to spatial orientation for the purpose of movement coordination. Vestibular disorders are incredibly common, and exhibit many different symptoms including vertigo, unsteadiness and navigation issues, but also emotional and social problems. Many of the assessment, training and rehabilitation approaches developed so far cannot guarantee the necessary degree of usability, measurability and repeatability. This paper presents the preparatory steps towards the design of a methodology for treating vestibular disorders that combines established methods with innovative, robot-based exergames to foster, among others, engagement and flexibility. Preliminary results obtained through a user study that involved non-pathological subjects offered helpful indications that could be exploited in the design and validation of novel rehabilitation protocols in the field. © 2018 IEEE.},
	author_keywords = {assessment; balance; exergames; gamification; rehabilitation; robotics; spatial orientation; vestibular disorders},
	keywords = {Balancing; Machine design; Mobile robots; Patient rehabilitation; Robotics; Ubiquitous computing; assessment; Exergames; Gamification; Spatial orientations; vestibular disorders; Mobile telecommunication systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Calandra2021380,
	author = {Calandra, Davide and Cannavo, Alberto and Lamberti, Fabrizio},
	title = {Evaluating an Augmented Reality-Based Partially Assisted Approach to Remote Assistance in Heterogeneous Robotic Applications},
	year = {2021},
	journal = {International Conference on Virtual Rehabilitation, ICVR},
	volume = {2021-May},
	pages = {380 – 387},
	doi = {10.1109/ICVR51878.2021.9483849},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111448111&doi=10.1109%2fICVR51878.2021.9483849&partnerID=40&md5=1af2231cbb73c25b7ff481e342e77d7d},
	abstract = {Among the countless applications of Augmented Reality (AR) in the industry, remote assistance represents one of the most prominent and widely studied use cases. Recently, the way in which assistance can be delivered started to evolve, unleashing the full potential of such technology. New methodologies have been proposed able to foster operators' autonomy and reduce under-utilization of skilled human resources. This paper studies the effectiveness of a recently proposed approach to AR-based remote assistance, referred to as partially assisted, which differs from the traditional step-by-step guidance in the way the AR hints are conveyed by the expert to the operator. The suitability of this approach has been proved already for a number of simple industrial tasks, but a comprehensive study has yet to be performed for validating its effectiveness in complex use cases. This paper addresses this lack by considering as a case study the mastering of a robotic manipulator, a procedure involving a number of heterogeneous operations. The performance of the partially assisted approach is compared with step-by-step guidance based on both objective and subjective metrics. Results showed that the former approach could be particularly effective in reducing the time investment for the expert, allowing the operator to autonomously complete the assigned task in a time comparable to traditional assistance with a negligible need for further support. © 2021 IEEE.},
	author_keywords = {augmented reality; autonomous operation; industrial applications; process efficiency; remote assistance},
	keywords = {Augmented reality; Industrial manipulators; Manipulators; Robotics; Virtual reality; Industrial tasks; Remote assistance; Robotic applications; Robotic manipulators; Air navigation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Pratticò2021,
	author = {Pratticò, Filippo Gabriele and De Lorenzis, Federico and Calandra, Davide and Cannavò, Alberto and Lamberti, Fabrizio},
	title = {Exploring simulation-based virtual reality as a mock-up tool to support the design of first responders training},
	year = {2021},
	journal = {Applied Sciences (Switzerland)},
	volume = {11},
	number = {16},
	doi = {10.3390/app11167527},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113294333&doi=10.3390%2fapp11167527&partnerID=40&md5=28cf3dd9d2cff784b4083ce41f0600b8},
	abstract = {Intervention by First Responders (FRs) is essential in disaster response, and their preparation greatly benefits from continuous updates. However, the design of effective training experiences targeted to FRs can be very demanding from the viewpoint of a Training Provisioner (TP). Virtual Reality (VR) may have a key role to play in enhancing and facilitating this task. In fact, VR technology has already proven to be very helpful in the field of emergency training, as well as its use as a powerful design and mock-up tool in many other contexts. In this work, the application of VR as a mock-up tool supporting TPs in the arrangement and validation of a training experience, either real or virtual, is explored. In particular, a case study is considered concerning the training of an FR for hydro-geological risks. Within this context, the proposed approach is compared against dramaturgy prototyping, a method commonly used for the design of experiential courses. Results indicate that the adoption of a VR-based mock-up tool (VRMT) can provide TPs with good insights on the arrangement of the training and precious indications on how to actually map this information onto real-world exercises. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Dramaturgy prototyping; Experiential courses; First Responders; Mock-up tool; Training design; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Prattico20211157,
	author = {Prattico, F. Gabriele and Lamberti, Fabrizio and Cannavo, Alberto and Morra, Lia and Montuschi, Paolo},
	title = {Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for Autonomous Vehicle-to-Pedestrian Communication},
	year = {2021},
	journal = {IEEE Transactions on Vehicular Technology},
	volume = {70},
	number = {2},
	pages = {1157 – 1168},
	doi = {10.1109/TVT.2021.3054312},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100451262&doi=10.1109%2fTVT.2021.3054312&partnerID=40&md5=5f30a4091b19a45243a2daad6a957174},
	abstract = {Providing pedestrians and other vulnerable road users with a clear indication about a fully autonomous vehicle status and intentions is crucial to make them coexist. In the last few years, a variety of external interfaces have been proposed, leveraging different paradigms and technologies including vehicle-mounted devices (like LED panels), short-range on-road projections, and road infrastructure interfaces (e.g., special asphalts with embedded displays). These designs were experimented in different settings, using mockups, specially prepared vehicles, or virtual environments, with heterogeneous evaluation metrics. Promising interfaces based on Augmented Reality (AR) have been proposed too, but their usability and effectiveness have not been tested yet. This paper aims to complement such body of literature by presenting a comparison of state-of-the-art interfaces and new designs under common conditions. To this aim, an immersive Virtual Reality-based simulation was developed, recreating a well-known scenario represented by pedestrians crossing in urban environments under non-regulated conditions. A user study was then performed to investigate the various dimensions of vehicle-to-pedestrian interaction leveraging objective and subjective metrics. Even though no interface clearly stood out over all the considered dimensions, one of the AR designs achieved state-of-the-art results in terms of safety and trust, at the cost of higher cognitive effort and lower intuitiveness compared to LED panels showing anthropomorphic features. Together with rankings on the various dimensions, indications about advantages and drawbacks of the various alternatives that emerged from this study could provide important information for next developments in the field.  © 1967-2012 IEEE.},
	author_keywords = {augmented reality; Fully autonomous vehicles; human-machine interaction; pedestrian crossing; vehicle-to-pedestrian communication; virtual reality},
	keywords = {Augmented reality; Display devices; Interface states; Light emitting diodes; Pedestrian safety; Road vehicles; Roads and streets; Virtual reality; Cognitive efforts; Evaluation metrics; Fully-autonomous vehicles; Immersive virtual reality; Information sources; Reality interface; Road infrastructures; Urban environments; Autonomous vehicles},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 44; All Open Access, Green Open Access}
}

@ARTICLE{Cannavò2024,
	author = {Cannavò, Alberto and Bottino, Francesco and Lamberti, Fabrizio},
	title = {Supporting motion-capture acting with collaborative Mixed Reality},
	year = {2024},
	journal = {Computers and Graphics (Pergamon)},
	volume = {124},
	doi = {10.1016/j.cag.2024.104090},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204675352&doi=10.1016%2fj.cag.2024.104090&partnerID=40&md5=da75b395f07d9a7661db0fda4f8194f0},
	abstract = {Technologies such as chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are revolutionizing how films featuring visual effects are produced. Despite their popularity, these technologies have introduced new challenges for actors. An increased workload is faced when digital characters are animated via mocap, since actors are requested to use their imagination to envision what characters see and do on set. This work investigates how Mixed Reality (MR) technology can support actors during mocap sessions by presenting a collaborative MR system named CoMR-MoCap, which allows actors to rehearse scenes by overlaying digital contents onto the real set. Using a Video See-Through Head Mounted Display (VST-HMD), actors can see digital representations of performers in mocap suits and digital scene contents in real time. The system supports collaboration, enabling multiple actors to wear both mocap suits to animate digital characters and VST-HMDs to visualize the digital contents. A user study involving 24 participants compared CoMR-MoCap to the traditional method using physical props and visual cues. The results showed that CoMR-MoCap significantly improved actors’ ability to position themselves and direct their gaze, and it offered advantages in terms of usability, spatial and social presence, embodiment, and perceived effectiveness over the traditional method. © 2024},
	author_keywords = {Acting rehearsal and performance; Collaborative virtual production; Mixed reality; Motion capture; Visual effects},
	keywords = {Head-up displays; Helmet mounted displays; Layered manufacturing; Mixed reality; Acting rehearsal and performance; Collaborative virtual production; Digital characters; Digital contents; Mixed reality; Motion capture; Performance; Virtual production; Visual effects; Wall motion; Motion capture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Cannavò2017266,
	author = {Cannavò, Alberto and Cermelli, Fabio and Chiaramida, Vincenzo and Ciccone, Giovanni and Lamberti, Fabrizio and Montuschi, Paolo and Paravati, Gianluca},
	title = {T4T: Tangible interface for tuning 3D object manipulation tools},
	year = {2017},
	journal = {2017 IEEE Symposium on 3D User Interfaces, 3DUI 2017 - Proceedings},
	pages = {266 – 267},
	doi = {10.1109/3DUI.2017.7893374},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018996805&doi=10.1109%2f3DUI.2017.7893374&partnerID=40&md5=33965fa7f77837fcb59a4b0e2e07120a},
	abstract = {A 3D User Interface for manipulating virtual objects in Augmented Reality scenarios on handheld devices is presented. The proposed solution takes advantage of two interaction techniques. The former (named 'cursor mode') exploits a cursor, which position and movement are bound to the view of the device; the cursor allows the user to select objects and to perform coarse-grain manipulations by moving the device. The latter (referred to as 'tuning mode') uses the physical affordances of a tangible interface to provide the user with the possibility to refine objects in all their aspects (position, rotation, scale, color, and so forth) with a fine-grained control. © 2017 IEEE.},
	author_keywords = {3D User Interfaces; Augmented Reality; Mobile Devices; Tangible Interfaces},
	keywords = {Augmented reality; Interfaces (computer); Mobile devices; 3D object manipulations; 3D user interface; Coarse grains; Fine-grained control; Hand held device; Interaction techniques; Tangible interfaces; Virtual objects; User interfaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Bologna2023941,
	author = {Bologna, Daniel and Micciché, Vincenzo and Violo, Giovanni and Visconti, Alessandro and Cannavò, Alberto and Lamberti, Fabrizio},
	title = {SPHinX Authentication Technique: Secure Painting autHentication in eXtended reality},
	year = {2023},
	journal = {Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023},
	pages = {941 – 942},
	doi = {10.1109/VRW58643.2023.00313},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159647609&doi=10.1109%2fVRW58643.2023.00313&partnerID=40&md5=50608a3d3f25c84092697f65c58235f4},
	abstract = {Authentication in immersive virtual reality, and more in general, into extended reality applications is a growing need. The crude adoption of 2D authentication schemes into virtual environments could raise security and usability issues. This paper proposes SPHinX, a novel three-dimensional (3D) authentication scheme based on unlocking patterns. Users can authenticate by painting (tracing) patterns on a 3D object. In this way, 3D interactions can be leveraged to make the authentication scheme less vulnerable to different types of attacks, e.g., shoulder surfing. A user study was carried out by involving 16 participants with the aim to compare a number of alternatives concerning the shape of the object to be painted, the use of single or multiple patterns, and multi-factor authentication.  © 2023 IEEE.},
	keywords = {Authentication; 3D interactions; 3D object; Authentication scheme; Authentication techniques; Immersive virtual reality; Multiple patterns; Pattern factors; Security and usabilities; Shoulder surfing; User study; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Cannavò201845,
	author = {Cannavò, Alberto and Lamberti, Fabrizio},
	title = {A virtual character posing system based on reconfigurable tangible user interfaces and immersive virtual reality},
	year = {2018},
	journal = {Italian Chapter Conference 2018 - Smart Tools and Apps in Computer Graphics, STAG 2018},
	pages = {45 – 55},
	doi = {10.2312/stag.20181297},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072553973&doi=10.2312%2fstag.20181297&partnerID=40&md5=06d143b4e02da4968c420827d122f46c},
	abstract = {Computer animation and, particularly, virtual character animation, are very time consuming and skill-intensive tasks, which require animators to work with sophisticated user interfaces. Tangible user interfaces (TUIs) already proved to be capable of making character animation more intuitive, and possibly more efficient, by leveraging the affordances provided by physical props that mimic the structure of virtual counterparts. The main downside of existing TUI-based animation solutions is the reduced accuracy, which is due partly to the use of mechanical parts, partly to the fact that, despite the adoption of a 3D input, users still have to work with a 2D output (usually represented by one or more views displayed on a screen). However, output methods that are natively 3D, e.g., based on virtual reality (VR), have been already exploited in different ways within computer animation scenarios. By moving from the above considerations and by building upon an existing work, this paper proposes a VR-based character animation system that combines the advantages of TUIs with the improved spatial awareness, enhanced visualization and better control on the observation point in the virtual space ensured by immersive VR. Results of a user study with both skilled and unskilled users showed a marked preference for the devised system, which was judged as more intuitive than that in the reference work, and allowed users to pose a virtual character in a lower time and with a higher accuracy. © 2018 The Eurographics Association.},
	keywords = {Animation; Arts computing; User interfaces; Based animations; Character animation; Computer animation; Immersive virtual reality; Observation point; Spatial awareness; Tangible user interfaces; Virtual character; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Cannavo20223397,
	author = {Cannavo, Alberto and Lamberti, Fabrizio and Thapliyal, Himanshu and Thawonmas, Ruck},
	title = {Guest Editorial Introduction to the Special Section on Immersive Virtual Reality Simulation for Vehicular Technology},
	year = {2022},
	journal = {IEEE Transactions on Vehicular Technology},
	volume = {71},
	number = {4},
	pages = {3397 – 3398},
	doi = {10.1109/TVT.2022.3160625},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132515937&doi=10.1109%2fTVT.2022.3160625&partnerID=40&md5=373df908bfacc12bd3de859f4d1d84fb},
	type = {Editorial},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Bronze Open Access}
}

@ARTICLE{Cannavò202051,
	author = {Cannavò, Alberto and Zhang, Congyi and Wang, Wenping and Lamberti, Fabrizio},
	title = {Posing 3D Characters in Virtual Reality Through In-the-Air Sketches},
	year = {2020},
	journal = {Communications in Computer and Information Science},
	volume = {1300},
	pages = {51 – 61},
	doi = {10.1007/978-3-030-63426-1_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097641575&doi=10.1007%2f978-3-030-63426-1_6&partnerID=40&md5=a3e1bfab476e5a4d1075b5aa72268f25},
	abstract = {Generating computer animations is a very labor-intensive task, which requires animators to operate with sophisticated interfaces. Hence, researchers continuously experiment with alternative interaction paradigms that could possibly ease the above task. Among others, sketching represents a valid alternative to traditional interfaces since it can make interactions more expressive and intuitive; however, although the literature proposes several solutions leveraging sketch-based interfaces to solve different computer graphics challenges, generally they are not fully integrated in the computer animation pipeline. At the same time, Virtual Reality (VR), is becoming commonplace in many domains, and recently started to be recognized as capable to make it easier also the animators’ job, by improving their spatial understanding of the animated scene and providing them with interfaces characterized by higher usability and effectiveness. Based on all of the above, this paper presents an add-on for a well-known animation suite that combines the advantages offered by a sketch-based interface and VR to let animators define poses and create virtual character animations in an immersive environment. © 2020, Springer Nature Switzerland AG.},
	author_keywords = {3D Animation; Sketch-based interfaces; Virtual reality},
	keywords = {Animation; Arts computing; 3D characters; Computer animation; Fully integrated; Immersive environment; Interaction paradigm; Labor intensive; Sketch based Interface; Virtual character; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Cannavò2019310,
	author = {Cannavò, Alberto and Calandra, Davide and Basilicò, Gianpaolo and Lamberti, Fabrizio},
	title = {Automatic recognition of sport events from spatio-temporal data: An application for virtual reality-based training in basketball},
	year = {2019},
	journal = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
	volume = {1},
	pages = {310 – 316},
	doi = {10.5220/0007524203100316},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068224262&doi=10.5220%2f0007524203100316&partnerID=40&md5=47a523ad877d7a6c7f7ccb66754f702a},
	abstract = {Data analysis in the field of sport is growing rapidly due to the availability of datasets containing spatio-temporal positional data of the players and other sport equipment collected during the game. This paper investigates the use of machine learning for the automatic recognition of small-scale sport events in a basketball-related dataset. The results of the method discussed in this paper have been exploited to extend the functionality of an existing Virtual Reality (VR)-based tool supporting training in basketball. The tool allows the coaches to draw game tactics on a touchscreen, which can be then visualized and studies in an immersive VR environment by multiple players. Events recognized by the proposed system can be used to let the tool manage also previous matches, which can be automatically recreated by activating different animations for the virtual players and the ball based on the particular game situation, thus increasing the realism of the simulation. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
	author_keywords = {Event Recognition; Machine Learning; Sport Training; Virtual Reality},
	keywords = {Animation; Computer vision; Learning systems; Machine learning; Sports; Virtual reality; Automatic recognition; Event recognition; Immersive VR; Spatio temporal; Spatio-temporal data; Sport events; Tool supporting; Virtual player; E-learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Cannavo20216,
	author = {Cannavo, Alberto and Lamberti, Fabrizio},
	title = {How Blockchain, Virtual Reality, and Augmented Reality are Converging, and Why},
	year = {2021},
	journal = {IEEE Consumer Electronics Magazine},
	volume = {10},
	number = {5},
	pages = {6 – 13},
	doi = {10.1109/MCE.2020.3025753},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091949011&doi=10.1109%2fMCE.2020.3025753&partnerID=40&md5=d1455b63ea56f641b3c1834e78d87ddc},
	abstract = {Nowadays, breakthrough technologies, such as virtual reality (VR), augmented reality (AR), and Blockchain, have definitively attracted the attention of a huge number of investors worldwide. Although, at first glance, Blockchain (traditionally used for financial services) seems to have little to none to share with VR and AR (originally adopted for entertainment), in the last few years several use cases started to appear showing effective ways to integrate these technologies. In this article, an overview of opportunities investigated by current solutions combining VR, AR, and Blockchain will be discussed, showing how they allowed both companies and academic researchers cope with issues affecting traditional services and products in a rather heterogenous set of application domains. Opportunities that could foster the convergence of these technologies and boost them further are also discussed. © 2012 IEEE.},
	keywords = {Augmented reality; Virtual reality; Breakthrough technology; Financial service; Traditional services; Blockchain},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40}
}

@ARTICLE{Cannavò2020,
	author = {Cannavò, Alberto and De Pace, Francesco and Salaroglio, Federico and Lamberti, Fabrizio},
	title = {A visual editing tool supporting the production of 3D interactive graphics assets for public exhibitions},
	year = {2020},
	journal = {International Journal of Human Computer Studies},
	volume = {141},
	doi = {10.1016/j.ijhcs.2020.102450},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084080088&doi=10.1016%2fj.ijhcs.2020.102450&partnerID=40&md5=429dd310b84d535f0d3b815765bf95a3},
	abstract = {The introduction of interactive assets in public exhibitions is capable to significantly enhance the visitors’ user experience. However, the creation of interactive applications could represent a challenging task, especially for users lacking computer skills. Visual programming languages (VPLs) – one of the instruments belonging to the broad categories of methods and tools devised to support end-user development (EUD) – promise to offer an intuitive way to overcome these limitations, by providing easy-to-use and efficient interfaces for encoding applications’ logic. Moving from these considerations, this paper first analyses pros and cons of tools devised so far to support the generation of interactive contents. Then, it presents the design of a new tool named Visual Scene Editor (VSE), which allows users with little to no programming skills to create 3D interactive applications by combining available assets through an interactive, visual process. Both objective and subjective measurements have been collected with both skilled and unskilled users to evaluate the performance of the proposed tool. A comparison with existing solutions shows a reduction in the time required to complete the assigned tasks, of the complexity of the logic created, as well as of the number of errors made, confirming the suitability of the VSE for the said purpose. © 2020 Elsevier Ltd},
	author_keywords = {3D Graphics; Augmented reality (AR); End-user development (EUD); Human-machine interaction (HMI); Interactive assets; Natural user interfaces (NUIs); Virtual reality (VR); Visual programming languages (VPLs)},
	keywords = {Computer circuits; Exhibitions; Interactive devices; Three dimensional computer graphics; User experience; 3D interactive graphics; Computer skill; End user development(EUD); Interactive applications; Interactive contents; Objective and subjective measurements; Programming skills; Visual programming languages; Visual languages},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Lamberti20191655,
	author = {Lamberti, Fabrizio and Gatteschi, Valentina and Sanna, Andrea and Cannavò, Alberto},
	title = {A Multimodal Interface for Virtual Character Animation Based on Live Performance and Natural Language Processing},
	year = {2019},
	journal = {International Journal of Human-Computer Interaction},
	volume = {35},
	number = {18},
	pages = {1655 – 1671},
	doi = {10.1080/10447318.2018.1561068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059750274&doi=10.1080%2f10447318.2018.1561068&partnerID=40&md5=5beb8a70a314e10f4e39b193ecc71b90},
	abstract = {Virtual character animation is receiving an ever-growing attention by researchers, who proposed already many tools with the aim to improve the effectiveness of the production process. In particular, significant efforts are devoted to create animation systems suited also to non-skilled users, in order to let them benefit from a powerful communication instrument that can improve information sharing in many contexts like product design, education, marketing, etc. Apart from methods based on the traditional Windows-Icons-Menus-Pointer (WIMP) paradigms, solutions devised so far leverage approaches based on motion capture/retargeting (the so-called performance-based approaches), on non-conventional interfaces (voice inputs, sketches, tangible props, etc.), or on natural language processing (NLP) over text descriptions (e.g., to automatically trigger actions from a library). Each approach has its drawbacks, though. Performance-based methods are difficult to use for creating non-ordinary movements (flips, handstands, etc.); natural interfaces are often used for rough posing, but results need to be later refined; automatic techniques still produce poorly realistic animations. To deal with the above limitations, we propose a multimodal animation system that combines performance- and NLP-based methods. The system recognizes natural commands (gestures, voice inputs) issued by the performer, extracts scene data from a text description and creates live animations in which pre-recorded character actions can be blended with performer’s motion to increase naturalness. © 2018, © 2018 Taylor & Francis Group, LLC.},
	keywords = {Animation; Character recognition; Product design; Automatic technique; Information sharing; Multi-modal interfaces; Multimodal animations; Natural interfaces; Performance based approach; Performance based method; Production process; Natural language processing systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Cannavò20211871,
	author = {Cannavò, Alberto and Calandra, Davide and Pratticò, F. Gabriele and Gatteschi, Valentina and Lamberti, Fabrizio},
	title = {An Evaluation Testbed for Locomotion in Virtual Reality},
	year = {2021},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = {27},
	number = {3},
	pages = {1871 – 1889},
	doi = {10.1109/TVCG.2020.3032440},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100383262&doi=10.1109%2fTVCG.2020.3032440&partnerID=40&md5=815bc3e8c0bed98df32e385fca283398},
	abstract = {A common operation performed in Virtual Reality (VR) environments is locomotion. Although real walking can represent a natural and intuitive way to manage displacements in such environments, its use is generally limited by the size of the area tracked by the VR system (typically, the size of a room) or requires expensive technologies to cover particularly extended settings. A number of approaches have been proposed to enable effective explorations in VR, each characterized by different hardware requirements and costs, and capable to provide different levels of usability and performance. However, the lack of a well-defined methodology for assessing and comparing available approaches makes it difficult to identify, among the various alternatives, the best solutions for selected application domains. To deal with this issue, this article introduces a novel evaluation testbed which, by building on the outcomes of many separate works reported in the literature, aims to support a comprehensive analysis of the considered design space. An experimental protocol for collecting objective and subjective measures is proposed, together with a scoring system able to rank locomotion approaches based on a weighted set of requirements. Testbed usage is illustrated in a use case requesting to select the technique to adopt in a given application scenario. © 2020 IEEE.},
	author_keywords = {evaluation; locomotion; performance; requirements; testbed; user experience; virtual environments; Virtual reality},
	keywords = {Testbeds; Application scenario; Common operations; Comprehensive analysis; Design spaces; Experimental protocols; Objective and subjective measures; Scoring systems; Weighted set; article; human; human experiment; locomotion; outcome assessment; scoring system; virtual reality; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 33; All Open Access, Green Open Access}
}

@CONFERENCE{Attanasio2017256,
	author = {Attanasio, Giuseppe and Cannavò, Alberto and Cibrario, Francesca and Lamberti, Fabrizio and Montuschi, Paolo and Paravati, Gianluca},
	title = {HOT: Hold your own tools for AR-based constructive art},
	year = {2017},
	journal = {2017 IEEE Symposium on 3D User Interfaces, 3DUI 2017 - Proceedings},
	pages = {256 – 257},
	doi = {10.1109/3DUI.2017.7893369},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018997686&doi=10.1109%2f3DUI.2017.7893369&partnerID=40&md5=965bdbfacd2a3a18f27491550bc6b0c5},
	abstract = {Using digital instruments to support artistic expression and creativity is a hot topic. In this work, we focused on the design of a suitable interface for Augmented Reality-based constructive art on handheld devices. Issues to be faced encompassed how to give artists sense of spatial dimensions, how to provide them with different tools for realizing artworks, and how much moving away from 'the real' and going towards 'the virtual'. Through a touch-capable device, such as a smartphone or a tablet, we offer artists a clean workspace, where they can decide when to introduce artworks and tools. In fact, besides exploiting the multi-touch functionality and the gyroscopes/accelerometers to manipulate artworks in six degrees of freedom (6DOF), the proposed solution exploits a set of printed markers that can be brought into the camera's field of view to make specific virtual tools appear in the augmented scene. With such tools, artists can decide to control, e.g., manipulation speed, scale factor, scene parameters, etc., thus complementing functionalities that can be accessed via the device's screen. © 2017 IEEE.},
	author_keywords = {3D User Interfaces; Augmented Reality; Constructive Art; Customized Tags; Mobile Devices},
	keywords = {Arts computing; Augmented reality; Degrees of freedom (mechanics); Mobile devices; 3D user interface; Constructive Art; Customized Tags; Field of views; Hand held device; Six degrees of freedom; Spatial dimension; Virtual tool; User interfaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Calandra2023985,
	author = {Calandra, Davide and De Lorenzis, Federico and Cannavò, Alberto and Lamberti, Fabrizio},
	title = {Immersive virtual reality and passive haptic interfaces to improve procedural learning in a formal training course for first responders},
	year = {2023},
	journal = {Virtual Reality},
	volume = {27},
	number = {2},
	pages = {985 – 1012},
	doi = {10.1007/s10055-022-00704-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139668282&doi=10.1007%2fs10055-022-00704-9&partnerID=40&md5=260c0204242b5ec7d035e9b610a2caca},
	abstract = {One key aspect for the safety and success of first responders’ operations is the compliance, during the intervention, with all the safety procedures and prescribed behaviors. Although real-world simulation exercises are considered as the best way to verify if operators are ready to handle emergency situations, they are not always a viable approach. Firefighting courses, for example, do not usually include this kind of activities, due to the numerous hazards related to deploying controlled fires for the simulation. However, traditional training approaches based on class lessons and multimedia learning material may not be particularly effective for teaching practical skills and procedural behaviors. In this work, the use of a Virtual Reality Training Simulation (VRTS) combined with passive haptic interfaces and a real-time fire simulation logic is investigated as a complement to a traditional video-based training approach used in the context of forest firefighting. The teaching of safety concepts and correct use of individual firefighting tools was selected as a use case, and a user study involving 45 trainees was carried out in the context of an existing training course. One third of the trainees attended the traditional video-based lessons of the course, whereas the remaining ones also took part to a practice training session, half of them with the devised VRTS, the others in the real world. Experimental results showed that the additional use of the devised VRTS improved the trainees’ procedural learning, as well as their motivation and perceived quality of the overall learning experience. © 2022, The Author(s).},
	author_keywords = {Fire simulation; First responders; Forest firefighting; Passive haptics; Video-based training; Virtual reality},
	keywords = {E-learning; Fire extinguishers; Fires; Forestry; Haptic interfaces; Teaching; Fire simulation; First responders; Forest firefighting; Haptics; Haptics interfaces; Passive haptic; Procedural learning; Training simulation; Video-based training; Virtual reality training; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Cannavò2021291,
	author = {Cannavò, Alberto and Calandra, Davide and Kehoe, Aidan and Lamberti, Fabrizio},
	title = {Evaluating Consumer Interaction Interfaces for 3D Sketching in Virtual Reality},
	year = {2021},
	journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
	volume = {367 LNICST},
	pages = {291 – 306},
	doi = {10.1007/978-3-030-73426-8_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104790713&doi=10.1007%2f978-3-030-73426-8_17&partnerID=40&md5=8b9da78b2b41c24ba0675df13864d075},
	abstract = {Since its introduction, 3D mid-air sketching in immersive Virtual Reality (VR) proved to be a very powerful tool for many creative applications. However, common VR sketching suites rely on the standard hand controllers bundled with home VR systems, which are non-optimal for this kind of tasks. To deal with this issue, some research works proposed to use dedicated pen-shaped interfaces tracked with external motion-capture systems. Regrettably, these solutions are generally rather expensive, cumbersome and unsuitable for many potential end-users. Hence, lots of challenges regarding interfaces for 3D sketching in VR still exist. In this paper, a newly proposed sketching-oriented input device (namely, a VR stylus) compatible with the tracking technology of a consumer-grade VR system is compared with a standard hand controller from the same system. In particular, the paper reports the results of a user study whose aim was to evaluate, in both objective and subjective terms, aspects like, among others, sketching accuracy, ease of use, efficiency, comfort, control and naturalness. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
	author_keywords = {3D sketching; Human-computer interaction; Virtual Reality; VR stylus},
	keywords = {3D sketching; Consumer interaction; Ease-of-use; Immersive virtual reality; Input devices; Motion capture system; Tracking technology; User study; Virtual reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@CONFERENCE{Calandra20191403,
	author = {Calandra, Davide and Prattico, F. Gabriele and Cannavo, Alberto and Micelli, Luca and Lamberti, Fabrizio},
	title = {Building reconfigurable passive haptic interfaces on demand using off-the-shelf construction bricks},
	year = {2019},
	journal = {26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings},
	pages = {1403 – 1404},
	doi = {10.1109/VR.2019.8797865},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071852603&doi=10.1109%2fVR.2019.8797865&partnerID=40&md5=015b13db3ac40172c0566ecf58ae4c94},
	abstract = {Although passive haptic interfaces have been shown to be capable to enhance the user's sense of presence in Mixed Reality experiences, their use is still constrained by the need to rely on exact replicas of virtual objects or on custom-made devices mimicking the original ones. Unfortunately, the former are not flexible enough in terms of reconfigurability, whereas the latter may be difficult to reproduce. To tackle these issues, this paper explores the possibility to build passive haptic interfaces using off-the-shelf toy construction bricks. Bricks can be assembled to provide the intended feedback in more than one task. Moreover, they may be reassembled in another application to mimic completely new objects and support totally different tasks. © 2019 IEEE.},
	author_keywords = {Centered computing; Communication hardware; Haptic devices; Hardware; Human; Human computer interaction (HCI); Interaction paradigms; Interfaces and storage; Tactile and handbased interfaces; Virtual reality},
	keywords = {Brick; Computer hardware; Human computer interaction; Mice (computer peripherals); Mixed reality; Virtual reality; Centered computing; Communication hardware; Haptic devices; Human; Human computer interaction (HCI); Interaction paradigm; Haptic interfaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Bronze Open Access}
}

@ARTICLE{Lamberti20171989,
	author = {Lamberti, Fabrizio and Paravati, Gianluca and Gatteschi, Valentina and Cannavo, Alberto},
	title = {Supporting web analytics by aggregating user interaction data from heterogeneous devices using viewport-DOM-based heat maps},
	year = {2017},
	journal = {IEEE Transactions on Industrial Informatics},
	volume = {13},
	number = {4},
	pages = {1989 – 1999},
	doi = {10.1109/TII.2017.2658663},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029360675&doi=10.1109%2fTII.2017.2658663&partnerID=40&md5=64bc978845b8dd5269a0f16e854b6be4},
	abstract = {The players of the digital industry look at network Big Data as an incredible source of revenues, which can allow them to design products, services, and market strategies ever more tailored to users' interests and needs. This is the case of data collected by Web analytics tools, which describe the way users interact with Web contents and where their attention focuses onto during navigation. Given the complexity of information to analyze, existing tools often make use of visualization strategies to represent data aggregated throughout separate sessions and multiple users. In particular, heat maps are often adopted to study the distribution of mouse activity and identify page regions that are more frequently reached during interaction. Unfortunately, since Web contents are accessed via ever more heterogeneous devices, region-based heat maps cannot be exploited anymore to aggregate data concerning user's attention, since the same Web content may move to another page location or exhibit a different aspect depending on the access device used or the user agent setup. This paper presents the design of a visual analytics framework capable to deal with the above limitation by adopting a data collection approach that combines information about regions displayed with information about page structure. This way, the well-known heat map-based visualization can be produced, where interactions can be aggregated on a per-element basis independently of the specific access configuration. Experimental results showed that the framework succeeds in accurately quantifying user's attention and replicating results obtained by manual processing. © 2005-2012 IEEE.},
	author_keywords = {Data visualization; heat maps; interaction patterns; network Big Data; user's attention; Web analytics; Web navigation},
	keywords = {Data visualization; Product design; User interfaces; Visualization; Websites; Heat maps; Interaction pattern; user's attention; Web analytics; Web navigation; Big data},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access, Green Open Access}
}

@ARTICLE{Cannavò20241811,
	author = {Cannavò, Alberto and Castiello, Antonio and Pratticò, F. Gabriele and Mazali, Tatiana and Lamberti, Fabrizio},
	title = {Immersive movies: the effect of point of view on narrative engagement},
	year = {2024},
	journal = {AI and Society},
	volume = {39},
	number = {4},
	pages = {1811 – 1825},
	doi = {10.1007/s00146-022-01622-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146338700&doi=10.1007%2fs00146-022-01622-9&partnerID=40&md5=236ea61e6d53075dc40d2a6f8336d45c},
	abstract = {Cinematic virtual reality (CVR) offers filmmakers a wide range of possibilities to explore new techniques regarding movie scripting, shooting and editing. Despite the many experiments performed so far both with both live action and computer-generated movies, just a few studies focused on analyzing how the various techniques actually affect the viewers’ experience. Like in traditional cinema, a key step for CVR screenwriters and directors is to choose from which perspective the viewers will see the scene, the so-called point of view (POV). The aim of this paper is to understand to what extent watching an immersive movie from a specific POV could impact the narrative engagement (NE), i.e., the viewers’ sensation of being immersed in the movie environment and being connected with its characters and story. Two POVs that are typically used in CVR, i.e., first-person perspective (1-PP) and external perspective (EP), are investigated through a user study in which both objective and subjective metrics were collected. The user study was carried out by leveraging two live action 360° short films with distinct scripts. The results suggest that the 1-PP experience could be more pleasant than the EP one in terms of overall NE and narrative presence, or even for all the NE dimensions if the potential of that POV is specifically exploited. © The Author(s) 2023.},
	author_keywords = {360°; Cinematic VR; External perspective; First-person perspective; Immersive videos; Omnidirectional; User study},
	keywords = {Motion pictures; 360°; Cinematic VR; Cinematics; External perspective; First-person perspectives; Immersive; Immersive video; Live actions; Omnidirectional; User study; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Cannavò201753,
	author = {Cannavò, Alberto and Lamberti, Fabrizio},
	title = {User interaction feedback in a hand-controlled interface for robot team tele-operation using wearable augmented reality},
	year = {2017},
	journal = {Italian Chapter Conference 2017 - Smart Tools and Apps in computer Graphics, STAG 2017},
	pages = {53 – 62},
	doi = {10.2312/stag.20171227},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088355620&doi=10.2312%2fstag.20171227&partnerID=40&md5=eb79555ea86acfe3546876deb5fa6f18},
	abstract = {Continuous advancements in the field of robotics and its increasing spread across heterogeneous application scenarios make the development of ever more effective user interfaces for human-robot interaction (HRI) an extremely relevant research topic. In particular, Natural User Interfaces (NUIs), e.g., based on hand and body gestures, proved to be an interesting technology to be exploited for designing intuitive interaction paradigms in the field of HRI. However, the more sophisticated the HRI interfaces become, the more important is to provide users with an accurate feedback about the state of the robot as well as of the interface itself. In this work, an Augmented Reality (AR)-based interface is deployed on a head-mounted display to enable tele-operation of a remote robot team using hand movements and gestures. A user study is performed to assess the advantages of wearable AR compared to desktop-based AR in the execution of specific tasks. © 2017 The Author(s) Eurographics Proceedings © 2017 The Eurographics Association.},
	keywords = {Feedback; Helmet mounted displays; Human robot interaction; Interface states; User interfaces; Application scenario; Body gesture; Hand gesture; Humans-robot interactions; Interaction paradigm; Intuitive interaction; Research topics; Robot teams; Tele-operations; User interaction; Augmented reality},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cannavò2021109,
	author = {Cannavò, Alberto and Bardella, Christian and Semeraro, Lorenzo and De Lorenzis, Federico and Zhang, Congyi and Jiang, Ying and Lamberti, Fabrizio},
	title = {An Automatic 3D Scene Generation Pipeline Based on a Single 2D Image},
	year = {2021},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12980 LNCS},
	pages = {109 – 117},
	doi = {10.1007/978-3-030-87595-4_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115843279&doi=10.1007%2f978-3-030-87595-4_9&partnerID=40&md5=784371853368998da1f569299b1b0049},
	abstract = {In the last years, solutions were proposed in the literature to alleviate the complexity of using sophisticated graphic suites for 3D scene generation by leveraging automatic tools. The most common approach based on the processing of text descriptions, however, may not represent the ideal solution, e.g., for fast prototyping purposes. This paper proposes an alternative methodology able to extract information about the objects and the layout of the scene to be created from a single 2D image. Compared to previous works, experimental results reported in this work show improvements in terms of similarity between the 2D and 3D scenes. © 2021, Springer Nature Switzerland AG.},
	author_keywords = {Image-based modelling; Machine learning; Scene-object modelling},
	keywords = {3D modeling; Three dimensional computer graphics; 2D images; 3D scenes; Automatic tools; Fast prototyping; Ideal solutions; Image-based models; Object modeling; Scene generation; Scene object; Scene-object modeling; Machine learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bonaiuto2017555,
	author = {Bonaiuto, Stefano and Cannavò, Alberto and Piumatti, Giovanni and Paravati, Gianluca and Lamberti, Fabrizio},
	title = {Tele-operation of Robot Teams: A Comparison of Gamepad-, Mobile Device and Hand Tracking-Based User Interfaces},
	year = {2017},
	journal = {Proceedings - International Computer Software and Applications Conference},
	volume = {2},
	pages = {555 – 560},
	doi = {10.1109/COMPSAC.2017.278},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032868611&doi=10.1109%2fCOMPSAC.2017.278&partnerID=40&md5=ce8061188822e7c4f18ea1d59f0908c9},
	abstract = {Due to the continuous advancements made in robot technologies, the development of intuitive and effective user interfaces for human-robot interaction is getting increasingly important. This paper investigates how different types of interfaces can be used for allowing a single operator to remotely control a team of robots endowed with different capabilities. Attention is focused on three user interfaces based on a gamepad, on a mobile device and on hand tracking, respectively. To evaluate pros and cons of the above interfaces, a user study was conducted, in which participants had to combine the capabilities of a rover, a drone and a robotic arm in order to carry out a search and pick task. Based on the experiments, the fastest way to complete the task was to use the mobile device. However, results showed that some of the interfaces could provide better performances for selected robots and associated sub-tasks. It is worth observing that, despite evidences about efficiency, participants rated the gamepad as the preferred interface from the point of view of subjective usability. © 2017 IEEE.},
	keywords = {Application programs; Computer software; Human robot interaction; Mobile devices; Palmprint recognition; Robots; Hand tracking; Robot teams; Robot technology; Subtasks; User study; User interfaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Cannavò20241763,
	author = {Cannavò, Alberto and De Lorenzis, Federico and Pratticò, Filippo Gabriele and Galante, Lorenzo and Lamberti, Fabrizio},
	title = {On the Quality of the Experience With Virtual Reality-Based Instructional Tools for Science Lab Activities},
	year = {2024},
	journal = {Journal of Educational Computing Research},
	volume = {62},
	number = {7},
	pages = {1763 – 1797},
	doi = {10.1177/07356331241270658},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202772391&doi=10.1177%2f07356331241270658&partnerID=40&md5=a9476d11c50709d37221951b21bc9dcb},
	abstract = {Today, immersive technologies like Virtual Reality (VR) are regarded as disruptive tools in many domains, including education. While the body of literature in the field is growing, studies that present contrasting findings are not uncommon. In fact, although there is evidence of the benefits brought by VR in the educational processes, in some cases the effects of a possible trade-off between learning effectiveness and quality of the learning experience (or QoLE) may be observed. The two dimensions are difficult to disentangle, as besides learning effectiveness, other factors like motivation, technology acceptability, workload, presence, immersion, engagement, and usability come to play. This paper digs into the above scenario by focusing on the QoLE of immersive VR-based learning and comparing it with that of two conventional approaches (a physical prop-based one and a 3D desktop application). Separation of the two dimensions is pursued by imposing equality of the learning performance achieved with the three approaches, aiming at getting rid of possible confounding factors. From the results of the user study performed in the context of a STEM-related laboratory activity, the VR-based approach appeared to be generally superior to the prop-based approach and showed several advantages over the 3D desktop application. © The Author(s) 2024.},
	author_keywords = {laboratory activities; learning experience; learning performance; technology-aided education; virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{De Lorenzis2023,
	author = {De Lorenzis, Federico and Visconti, Alessandro and Marani, Martina and Prifti, Erik and Andiloro, Camilla and Cannavo, Alberto and Lamberti, Fabrizio},
	title = {3DK-Reate: Create Your Own 3D Key for Distributed Authentication in the Metaverse},
	year = {2023},
	journal = {2023 IEEE Gaming, Entertainment, and Media Conference, GEM 2023},
	doi = {10.1109/GEM59776.2023.10390314},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184809424&doi=10.1109%2fGEM59776.2023.10390314&partnerID=40&md5=78a874028a6d02590205ede6b15a9814},
	abstract = {The Metaverse is becoming a social and economic ecosystem in which users can meet each other, exchange personal data, and make transactions. In this respect, authentication can be easily forecasted to be one of the key aspects for the development of social, immersive Virtual Reality (VR) environments. Notwithstanding, there is still a lack of techniques that can authenticate the users while protecting their sensitive data and leveraging the intrinsic characteristics of VR experiences. In this scenario, the present paper introduces a novel authentication schema based on asymmetric cryptography. More specifically, the proposed approach supports the creation of breakout rooms in which only mutually authenticated users can access. Due to the adoption of the asymmetric cryptography schema, the users are requested to create and insert their personal and secret private keys. To this aim, the current paper proposes and investigates four different interfaces for creating and inserting the keys, leveraging the three-dimensionality of the immersive environment.  © 2023 IEEE.},
	author_keywords = {3D user interface; asymmetric cryptography; authentication; metaverse},
	keywords = {Cryptography; Sensitive data; User interfaces; Virtual reality; 3D user interface; Asymmetric cryptography; Distributed authentication; Immersive virtual reality; Intrinsic characteristics; Metaverses; Private key; Sensitive datas; Virtual reality experiences; Virtual-reality environment; Authentication},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Lamberti202036,
	author = {Lamberti, Fabrizio and Cannavo, Alberto and Montuschi, Paolo},
	title = {Is Immersive Virtual Reality the Ultimate Interface for 3D Animators?},
	year = {2020},
	journal = {Computer},
	volume = {53},
	number = {4},
	pages = {36 – 45},
	doi = {10.1109/MC.2019.2908871},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083449865&doi=10.1109%2fMC.2019.2908871&partnerID=40&md5=1617145ba7d49eac43aacacb470b8907},
	abstract = {Creating computer animations is a labor-intensive task. Existing virtual reality (VR)-based animation solutions offer only heterogeneous subsets of traditional tools' functionalities. We present an add-on for the Blender animation suite that enables users to switch between native and immersive VR-based interfaces and employ the latter to perform a representative set of tasks. © 1970-2012 IEEE.},
	keywords = {Animation; Blending; Based animations; Computer animation; Immersive virtual reality; Immersive VR; Labor intensive; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20}
}