<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Alberto Cannavò </title> <meta name="author" content="Alberto Cannavò"> <meta name="description" content="publications by categories in reversed chronological order."> <meta name="keywords" content="alberto cannavo, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?a3d57046b69ba0aa250650faca8cc77b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cann-alberto.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Alberto Cannavò </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò20241003" class="col-sm-8"> <div class="title">Virtual prototyping for the textile industry: a framework supporting the production of crocheted objects</div> <div class="author"> Alberto Cannavò, Massimo Gismondi, and Fabrizio Lamberti </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 0 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Over the last years, many progresses have been made in the field of virtual prototyping, pushed by the interest of industries and artisans. Especially in the context of the textile industry, the digitizing of the prototyping stage offers the possibility to validate the product design choices before committing to the market. This paper presents a framework for the virtual prototyping of crocheted objects. The core of the framework is an algorithm that is capable of generating the crocheting patterns for a given object and the corresponding instructions. The instructions are leveraged by the framework to visualize the 3D geometry of the object, and can be also used to craft it. Compared to previous works, the proposed algorithm combines a number of features (primarily, the use of parametric surfaces and the support for short rows) that can reduce the distortions in crafted object shape while also lowering computational cost; the algorithm is also able to consider material- and style-related information. The results of a comparison between the proposed algorithm and state-of-the-art approaches showed improved performance in terms of similarity of the generated shape with the target one, computation time, and appearance of the crafted object. © 2023 Informa UK Limited, trading as Taylor &amp; Francis Group.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò20243234" class="col-sm-8"> <div class="title">A Sketch-Based Interface for Facial Animation in Immersive Virtual Reality</div> <div class="author"> Alberto Cannavò, Emanuele Stellini, Congyi Zhang, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 1 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Creating facial animations using 3D computer graphics represents a very laborious and time-consuming task. Among the numberless approaches for animating faces, the use of blendshapes remains the most common solution because of their simplicity and the ability to produce high-quality results. This approach, however, is also characterized by important drawbacks. With the traditional animation suites, to select the blendshapes to be activated animators are generally requested to memorize the mapping between the blendshapes and the influenced mesh vertices; alternatively, they need to adopt a trial-and-error search within the whole library of available blendshapes. Moreover, the level of expressiveness that can be reached may be lower than expected; this is due to the fact that the possibility to apply transformations to vertices different than just linear translations and mechanisms for adding, e.g., exaggerations, are typically not integrated into the same animation environment. To tackle these issues, this article proposes an immersive virtual reality-based interface that leverages sketches for the direct manipulation of blendshapes. Animators can draw both linear and curved strokes, which are used to automatically extract information about the blendshape to be activated and its weight, the trajectories that associated vertices have to follow, as well as the timing of the overall animation. A user study was carried out with the aim of evaluating the proposed approach on several representative animations tasks. Both objective and subjective measurements were collected. Experimental results showed the benefits of the devised interface in terms of task completion time, animation accuracy, and usability. © 2023 Taylor &amp; Francis Group, LLC.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavo2024" class="col-sm-8"> <div class="title">A Semi-Automated Pipeline for the Creation of Virtual Fitting Room Experiences Featuring Motion Capture and Cloth Simulation</div> <div class="author"> Alberto Cannavo, Giacomo Offre, and Fabrizio Lamberti </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 0 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Technological advancements are prompting the digitization of many industries, including fashion. Many brands are exploring ways to enhance customers’ experience, e.g., offering new shopping-oriented services like Virtual Fitting Rooms (VFRs). However, there are still challenges that prevent customers from effectively using these tools for trying on digital garments. Challenges are associated with difficulties in obtaining high-fidelity reconstructions of body shapes and providing realistic visualizations of animated clothes following real-time customers’ movements. This paper tackles such lacks by proposing a semi-automated pipeline supporting the creation of VFR experiences by exploiting state-of-the-art techniques for the accurate description and reconstruction of customers’ 3D avatars, motion capture-based animation, as well as realistic garment design and simulation. A user study in which the resulting VFR experience was compared with those created with two existing tools showed the benefits of the devised solution in terms of usability, embodiment, model accuracy, perceived value, adoption and purchase intention. © 1981-2012 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Calandra2024315" class="col-sm-8"> <div class="title">Digital twin- and extended reality-based telepresence for collaborative robot programming in the 6G perspective</div> <div class="author"> Davide Calandra, F. Gabriele Pratticò, Alberto Cannavò, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Claudio Casetti, Fabrizio Lamberti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 8; All Open Access, Gold Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In the context of Industry 4.0, a paradigm shift from traditional industrial manipulators to Collaborative Robots (CRs) is ongoing, with the latter serving ever more closely humans as auxiliary tools in many production processes. In this scenario, continuous technological advancements offer new opportunities for further innovating robotics and other areas of next-generation industry. For example, 6G could play a prominent role due to its human-centric view of the industrial domains. In particular, its expected dependability features will pave the way for new applications exploiting highly effective Digital Twin (DT)- and eXtended Reality (XR)-based telepresence. In this work, a novel application for the above technologies allowing two distant users to collaborate in the programming of a CR is proposed. The approach encompasses demanding data flows (e.g., point cloud-based streaming of collaborating users and robotic environment), with network latency and bandwidth constraints. Results obtained by analyzing this approach from the viewpoint of network requirements in a setup designed to emulate 6G connectivity indicate that the expected performance of forthcoming mobile networks will make it fully feasible in principle. © 2022 Chongqing University of Posts and Telecommunications</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2024" class="col-sm-8"> <div class="title">Passive Haptics and Conversational Avatars for Interacting with Ancient Egypt Remains in High-Fidelity Virtual Reality Experiences</div> <div class="author"> Alberto Cannavò, Simona Pacchiotti, Nicola Retta, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Martina Terzoli, Roberta Spallone, Fabrizio Lamberti' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 0; All Open Access, Hybrid Gold Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>As extended reality continues to grow, new possibilities arise to provide users with novel ways to experience cultural heritage (CH). In particular, applications based on virtual reality (VR), such as virtual museums, have gained increasing popularity, since they can offer new ways for preserving and presenting CH content that are not feasible in physical museums. Despite the numerous benefits, the level of immersion and presence provided by VR experiences still present challenges that could hinder the effectiveness of this technology in the CH context. In this perspective, it is crucial to provide the users with high-fidelity experiences, in which also the interaction with the objects and the characters populating virtual environments are realistic and natural. This article focuses on this challenge and specifically investigates how the combined use of tangible and speech interfaces can help improve the overall experience. To this aim, a immersive VR experience is proposed, which allows the users to manipulate virtual objects belonging to a museum collection (in the specific case, Ancient Egypt remains) by physically operating on 3D printed replicas and to talk with a curator’s avatar to get explanations by using their voice. A user study was conducted to evaluate the impact of the considered interfaces on immersion, presence, user experience, usability, and intention to visit, comparing the richest configuration against simpler setups obtained by either removing the tangible interface, the speech interface, or both (and using only handheld controllers). The results show that the combined use of the two interfaces can effectively contribute at making the CH experience in VR more engaging. Copyright © 2024 held by the owner/author(s).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2025" class="col-sm-8"> <div class="title">Supporting motion-capture acting with collaborative Mixed Reality</div> <div class="author"> Alberto Cannavò, Francesco Bottino, and Fabrizio Lamberti </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 1; All Open Access, Hybrid Gold Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Technologies such as chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are revolutionizing how films featuring visual effects are produced. Despite their popularity, these technologies have introduced new challenges for actors. An increased workload is faced when digital characters are animated via mocap, since actors are requested to use their imagination to envision what characters see and do on set. This work investigates how Mixed Reality (MR) technology can support actors during mocap sessions by presenting a collaborative MR system named CoMR-MoCap, which allows actors to rehearse scenes by overlaying digital contents onto the real set. Using a Video See-Through Head Mounted Display (VST-HMD), actors can see digital representations of performers in mocap suits and digital scene contents in real time. The system supports collaboration, enabling multiple actors to wear both mocap suits to animate digital characters and VST-HMDs to visualize the digital contents. A user study involving 24 participants compared CoMR-MoCap to the traditional method using physical props and visual cues. The results showed that CoMR-MoCap significantly improved actors’ ability to position themselves and direct their gaze, and it offered advantages in terms of usability, spatial and social presence, embodiment, and perceived effectiveness over the traditional method. © 2024</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò20241811" class="col-sm-8"> <div class="title">Immersive movies: the effect of point of view on narrative engagement</div> <div class="author"> Alberto Cannavò, Antonio Castiello, F. Gabriele Pratticò, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tatiana Mazali, Fabrizio Lamberti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 3; All Open Access, Hybrid Gold Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Cinematic virtual reality (CVR) offers filmmakers a wide range of possibilities to explore new techniques regarding movie scripting, shooting and editing. Despite the many experiments performed so far both with both live action and computer-generated movies, just a few studies focused on analyzing how the various techniques actually affect the viewers’ experience. Like in traditional cinema, a key step for CVR screenwriters and directors is to choose from which perspective the viewers will see the scene, the so-called point of view (POV). The aim of this paper is to understand to what extent watching an immersive movie from a specific POV could impact the narrative engagement (NE), i.e., the viewers’ sensation of being immersed in the movie environment and being connected with its characters and story. Two POVs that are typically used in CVR, i.e., first-person perspective (1-PP) and external perspective (EP), are investigated through a user study in which both objective and subjective metrics were collected. The user study was carried out by leveraging two live action 360° short films with distinct scripts. The results suggest that the 1-PP experience could be more pleasant than the EP one in terms of overall NE and narrative presence, or even for all the NE dimensions if the potential of that POV is specifically exploited. © The Author(s) 2023.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò20241763" class="col-sm-8"> <div class="title">On the Quality of the Experience With Virtual Reality-Based Instructional Tools for Science Lab Activities</div> <div class="author"> Alberto Cannavò, Federico De Lorenzis, Filippo Gabriele Pratticò, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Lorenzo Galante, Fabrizio Lamberti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> Cited by: 1 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Today, immersive technologies like Virtual Reality (VR) are regarded as disruptive tools in many domains, including education. While the body of literature in the field is growing, studies that present contrasting findings are not uncommon. In fact, although there is evidence of the benefits brought by VR in the educational processes, in some cases the effects of a possible trade-off between learning effectiveness and quality of the learning experience (or QoLE) may be observed. The two dimensions are difficult to disentangle, as besides learning effectiveness, other factors like motivation, technology acceptability, workload, presence, immersion, engagement, and usability come to play. This paper digs into the above scenario by focusing on the QoLE of immersive VR-based learning and comparing it with that of two conventional approaches (a physical prop-based one and a 3D desktop application). Separation of the two dimensions is pursued by imposing equality of the learning performance achieved with the three approaches, aiming at getting rid of possible confounding factors. From the results of the user study performed in the context of a STEM-related laboratory activity, the VR-based approach appeared to be generally superior to the prop-based approach and showed several advantages over the 3D desktop application. © The Author(s) 2024.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò202369" class="col-sm-8"> <div class="title">A Framework for Animating Customized Avatars from Monocular Videos in Virtual Try-On Applications</div> <div class="author"> Alberto Cannavò, Roberto Pesando, and Fabrizio Lamberti </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 1 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Generating real-time animations for customized avatars is becoming of paramount importance, especially in Virtual Try-On applications. This technology allows customers to explore or “try on” products virtually. Despite the numerous benefits of this technology, there are some aspects that prevent its applicability in real scenarios. The first limitation regards the difficulties in generating expressive avatar animations. Moreover, potential customers usually expressed concerns regarding the fidelity of the animations. To overcome these two limitations, the current paper is aimed at presenting a framework for animating customized avatars based on state-of-the-art techniques. The focus of the proposed work mainly relies on aspects regarding the animation of the customized avatars. More specifically, the framework encompasses two components. The first one automatizes the operations needed for generating the data structures used for the avatar animation. This component assumes that the mesh of the avatar is described through the Sparse Unified Part-Based Human Representation (SUPR). The second component of the framework is designed to animate the avatar through motion capture by making use of the MediaPipe Holistic pipeline. Experimental evaluations were carried out aimed at assessing the solutions proposed for pose beautification and joint estimations. Results demonstrated improvements in the quality of the reconstructed animation from both an objective and subjective point of view. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="De Lorenzis202379" class="col-sm-8"> <div class="title">MetaLibrary: Towards Social Immersive Environments for Readers</div> <div class="author"> Federico De Lorenzis, Alessandro Visconti, Alberto Cannavò, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 2 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The continuous integration of cutting-edge technologies in various fields such as culture and education is leading institutions towards a radical digital evolution. This work studies how one of the key actors of these domains, i.e., libraries, could exploit the digital transformation to reaffirm their position as primary cultural institutions in today’s society. Indeed, there is consensus that, among technologies that could be used to modernize libraries and help them to reach a wider audience, there are Artificial Intelligence (AI) and immersive media like Virtual Reality (VR). In particular, VR has been used to create social platforms that users can join from remote, experiencing virtual environments (VEs) where they can share opinions and perform activities together, thus creating a digital community. In this context, MetaLibrary was created, an immersive VE designed to let readers socialize, attend events with authors, and receive suggestions about books to read from an AI-based recommender system. Deployment is in progress in the city of Turin, Italy. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò202394" class="col-sm-8"> <div class="title">IEEE VR 2023 Workshops: Workshop: 3D Reconstruction, Digital Twinning, and Simulation for Virtual Experiences (ReDigiTS 2023)</div> <div class="author"> Alberto Cannavò, Bill Kapralos, Sofia Seinfeld, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Filippo Gabriele Pratticò, Congyi Zhang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 2 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Today, Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) technologies are becoming fundamental across a wide variety of application domains. All these technologies fall under the Extended Reality (XR) spectrum. Areas in which XR technologies and, in particular, immersive experiences have been successfully introduced are expanding daily, from traditional areas, such as entertainment and video game production, to novel scenarios, such as industry, healthcare, smart cities, and autonomous vehicles, to mention a few. The spread of XR and VR is also driven by technological advancements that are making new ways of operating within virtual experiences easier, and sometimes even possible. In particular, the availability of new hardware and software tools make the use of ’3D reconstructions’, ’digital twins’ and ’simulations’ commonplace in many XR and VR applications. For example, the possibility to leverage networks with higher bandwidth than never before and lower latency makes it possible to transmit the amount of data needed for massive, real-time digital twins, whereas high-performance hardware enables the reconstruction of high-fidelity digital objects and environments, while lowering the time needed for high-demanding simulations. The aim of the 3D Reconstruction, Digital Twinning, and Simulation for Virtual Experiences (ReDigiTS) workshop is to bring together researchers, practitioners, educators, and students to share ideas and promote further work in this growing area of 3D reconstruction, digital twinning, and simulation in VR and related technologies. © 2023 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavo2023318" class="col-sm-8"> <div class="title">AR-MoCap: Using Augmented Reality to Support Motion Capture Acting</div> <div class="author"> Alberto Cannavo, Filippo Gabriele Prattico, Alberto Bruno, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 4 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Technology is disrupting the way films involving visual effects are produced. Chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are only a few examples of the many changes introduced in the cinema industry over the last years. Although these technologies are getting commonplace, they are presenting new, unexplored challenges to the actors. In particular, when mocap is used to record the actors’ movements with the aim of animating digital character models, an increase in the workload can be easily expected for people on stage. In fact, actors have to largely rely on their imagination to understand what the digitally created characters will be actually seeing and feeling. This paper focuses on this specific domain, and aims to demonstrate how Augmented Reality (AR) can be helpful for actors when shooting mocap scenes. To this purpose, we devised a system named AR-MoCap that can be used by actors for rehearsing the scene in AR on the real set before actually shooting it. Through an Optical See-Through Head-Mounted Display (OST-HMD), an actor can see, e.g., the digital characters of other actors wearing mocap suits overlapped in real-time to their bodies. Experimental results showed that, compared to the traditional approach based on physical props and other cues, the devised system can help the actors to position themselves and direct their gaze while shooting the scene, while also improving spatial and social presence, as well as perceived effectiveness. © 2023 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hogue2023" class="col-sm-8"> <div class="title">Executive Conference Chair Welcome</div> <div class="author"> Andrew Hogue, and Alberto Cannavo </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 0; All Open Access, Bronze Open Access </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Prattico202376" class="col-sm-8"> <div class="title">A Breakdown Study of a Mockup-Based Consumer Haptic Setup for Virtual Reality</div> <div class="author"> Filippo Gabriele Prattico, Alberto Cannavo, Davide Calandra, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 2 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Despite the extensive use of visual and audio feedback in virtual reality (VR) experiences, it is possible to acknowledge a still limited exploitation of haptic devices to recreate the sense of touch, especially at the consumer level. In order to support the high variety of haptic stimuli, commercial off-the-shelf devices may need to be used together, combining separate functionalities into a unique solution and compensating for possibly lacking the features of more sophisticated equipment. This article moves from the results of a previous study, which analyzed the impact that two haptic setups built using consumer VR gloves and user-prepared props can have on immersive experiences encompassing an active electromechanical tool (precisely, an electric screwdriver). The said study showed that a combined setup, consisting of a pair of vibrotactile gloves and a custom-made mockup of the screwdriver, could be the most effective from many perspectives. It did not isolate, however, the contribution of each setup component to the users’ experience. Thus, the current work operates a breakdown analysis of the reference setup by first identifying a set of simpler, downgraded configurations that could be obtained using the original components, and then evaluating their performance-sophistication tradeoff through a new comparative study. © 2022 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lamberti2023237" class="col-sm-8"> <div class="title">Guest Editorial of the Special Section on Consumer Electronics With Artificial Intelligence</div> <div class="author"> Fabrizio Lamberti, Alberto Cannavò, Zaixing He, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Lucia Pepa, Chia-Mu Yu' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 1; All Open Access, Bronze Open Access </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Bologna2023941" class="col-sm-8"> <div class="title">SPHinX Authentication Technique: Secure Painting autHentication in eXtended reality</div> <div class="author"> Daniel Bologna, Vincenzo Micciché, Giovanni Violo, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Alessandro Visconti, Alberto Cannavò, Fabrizio Lamberti' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 2 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Authentication in immersive virtual reality, and more in general, into extended reality applications is a growing need. The crude adoption of 2D authentication schemes into virtual environments could raise security and usability issues. This paper proposes SPHinX, a novel three-dimensional (3D) authentication scheme based on unlocking patterns. Users can authenticate by painting (tracing) patterns on a 3D object. In this way, 3D interactions can be leveraged to make the authentication scheme less vulnerable to different types of attacks, e.g., shoulder surfing. A user study was carried out by involving 16 participants with the aim to compare a number of alternatives concerning the shape of the object to be painted, the use of single or multiple patterns, and multi-factor authentication. © 2023 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Calandra2023985" class="col-sm-8"> <div class="title">Immersive virtual reality and passive haptic interfaces to improve procedural learning in a formal training course for first responders</div> <div class="author"> Davide Calandra, Federico De Lorenzis, Alberto Cannavò, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 16; All Open Access, Hybrid Gold Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>One key aspect for the safety and success of first responders’ operations is the compliance, during the intervention, with all the safety procedures and prescribed behaviors. Although real-world simulation exercises are considered as the best way to verify if operators are ready to handle emergency situations, they are not always a viable approach. Firefighting courses, for example, do not usually include this kind of activities, due to the numerous hazards related to deploying controlled fires for the simulation. However, traditional training approaches based on class lessons and multimedia learning material may not be particularly effective for teaching practical skills and procedural behaviors. In this work, the use of a Virtual Reality Training Simulation (VRTS) combined with passive haptic interfaces and a real-time fire simulation logic is investigated as a complement to a traditional video-based training approach used in the context of forest firefighting. The teaching of safety concepts and correct use of individual firefighting tools was selected as a use case, and a user study involving 45 trainees was carried out in the context of an existing training course. One third of the trainees attended the traditional video-based lessons of the course, whereas the remaining ones also took part to a practice training session, half of them with the devised VRTS, the others in the real world. Experimental results showed that the additional use of the devised VRTS improved the trainees’ procedural learning, as well as their motivation and perceived quality of the overall learning experience. © 2022, The Author(s).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="De Lorenzis2023" class="col-sm-8"> <div class="title">3DK-Reate: Create Your Own 3D Key for Distributed Authentication in the Metaverse</div> <div class="author"> Federico De Lorenzis, Alessandro Visconti, Martina Marani, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Erik Prifti, Camilla Andiloro, Alberto Cannavo, Fabrizio Lamberti' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Cited by: 2 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The Metaverse is becoming a social and economic ecosystem in which users can meet each other, exchange personal data, and make transactions. In this respect, authentication can be easily forecasted to be one of the key aspects for the development of social, immersive Virtual Reality (VR) environments. Notwithstanding, there is still a lack of techniques that can authenticate the users while protecting their sensitive data and leveraging the intrinsic characteristics of VR experiences. In this scenario, the present paper introduces a novel authentication schema based on asymmetric cryptography. More specifically, the proposed approach supports the creation of breakout rooms in which only mutually authenticated users can access. Due to the adoption of the asymmetric cryptography schema, the users are requested to create and insert their personal and secret private keys. To this aim, the current paper proposes and investigates four different interfaces for creating and inserting the keys, leveraging the three-dimensionality of the immersive environment. © 2023 IEEE.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Gatteschi202253" class="col-sm-8"> <div class="title">Comparing Algorithms for Aggressive Driving Event Detection Based on Vehicle Motion Data</div> <div class="author"> Valentina Gatteschi, Alberto Cannavo, Fabrizio Lamberti, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Lia Morra, Paolo Montuschi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> Cited by: 13 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Aggressive driving is one of the main causes of fatal crashes. Correctly identifying aggressive driving events still represents a challenge in the literature. Furthermore, datasets available for testing the proposed approaches have some limitations since they generally (a) include only a few types of events, (b) contain data collected with only one device, and (c) are generated in drives that did not fully consider the variety of road characteristics and/or driving conditions. The main objective of this work is to compare the performance of several state-of-The-Art algorithms for aggressive driving event detection (belonging to anomaly detection-, threshold-and machine learning-based categories) on multiple datasets containing sensors data collected with different devices (black-boxes and smartphones), on different vehicles and in different locations. A secondary objective is to verify whether smartphones could replace black-boxes in aggressive/non-Aggressive classification tasks. To this aim, we propose the AD^2 (Aggressive Driving Detection) dataset, which contains (i) data collected using multiple devices to evaluate their influence on the algorithm performance, (ii) geographical data useful to analyze the context in which the events occurred, (iii) events recorded in different situations, and (iv) events generated by traveling the same path with aggressive and non-Aggressive driving styles, in order to possibly separate the effects of driving style from those of road characteristics. Our experimental results highlighted the superiority of machine learning-based approaches and underlined the ability of smartphones to ensure a level of performance similar to that of black-boxes. © 1967-2012 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò202277" class="col-sm-8"> <div class="title">Automatic Generation of 3D Animations from Text and Images</div> <div class="author"> Alberto Cannavò, Valentina Gatteschi, Luca Macis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> Cited by: 0 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The understanding of information in a text description can be improved by visually accompanying it with images or videos. This opportunity is particularly relevant for books and other traditional instructional material. Videos or, more in general, (interactive) graphics contents, can help to increase the effectiveness of this material, by providing, e.g., an animated representation of the steps to be performed to carry out a given procedure. The generation of 3D animated contents, however, is still very labor-intensive and time-consuming. Systems able to speed up this process offering flexible and easy-to-use interfaces are becoming of paramount importance. Hence, this paper describes a system designed to automatically generate a computer graphics video by processing a text description and a set of associated images. The system combines Natural Language Processing and image analysis for extracting information needed to visually represent the procedure depicted in an instruction manual using 3D animations. It relies on a database of 3D models and preconfigured animations that are activated according to the information extracted from the said input. Moreover, by analyzing the images, the system can also generate new animations from scratch. Promising results have been obtained assessing the system performance in a specific use case focused on printers maintenance. © 2022, Springer Nature Switzerland AG.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Ferri2022916" class="col-sm-8"> <div class="title">SPE Selection Technique: A Projection-based Approach for Precise Object Interaction in Immersive Virtual Environments</div> <div class="author"> Riccardo Ferri, Alberto Cannavo, F. Gabriele Prattico, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> Cited by: 0 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Selecting objects has always represented one of the fundamental tasks required by the majority of immersive applications. Despite the key role played by selection techniques in the design of im-mersive experiences, there is not a dominant or a one-size-fits-all approach yet. In this scenario, the current paper proposes a novel selection technique, named Selection among Projected Elements, which focuses on the selection of distant, small and possibly close, objects. The devised approach was assessed in the context of a virtual heritage application that was specifically designed with the aim of stressing the capabilities of the SPE technique. © 2022 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavo20223397" class="col-sm-8"> <div class="title">Guest Editorial Introduction to the Special Section on Immersive Virtual Reality Simulation for Vehicular Technology</div> <div class="author"> Alberto Cannavo, Fabrizio Lamberti, Himanshu Thapliyal, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ruck Thawonmas' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> Cited by: 7; All Open Access, Bronze Open Access </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Calandra20213147" class="col-sm-8"> <div class="title">Improving AR-powered remote assistance: a new approach aimed to foster operator’s autonomy and optimize the use of skilled resources</div> <div class="author"> Davide Calandra, Alberto Cannavò, and Fabrizio Lamberti </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 18; All Open Access, Hybrid Gold Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Augmented reality (AR) has a number of applications in industry, but remote assistance represents one of the most prominent and widely studied use cases. Notwithstanding, although the set of functionalities supporting the communication between remote experts and on-site operators grew over time, the way in which remote assistance is delivered has not evolved yet to unleash the full potential of AR technology. The expert typically guides the operator step-by-step, and basically uses AR-based hints to visually support voice instructions. With this approach, skilled human resources may go under-utilized, as the time an expert invests in the assistance corresponds to the time needed by the operator to execute the requested operations. The goal of this work is to introduce a new approach to remote assistance that takes advantage of AR functionalities separately proposed in academic works and commercial products to re-organize the guidance workflow, with the aim to increase the operator’s autonomy and, thus, optimize the use of expert’s time. An AR-powered remote assistance platform able to support the devised approach is also presented. By means of a user study, this approach was compared to traditional step-by-step guidance, with the aim to estimate what is the potential of AR that is still unexploited. Results showed that with the new approach it is possible to reduce the time investment for the expert, allowing the operator to autonomously complete the assigned tasks in a time comparable to step-by-step guidance with a negligible need for further support. © 2021, The Author(s).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jiang2021" class="col-sm-8"> <div class="title">Handpainter - 3d sketching in vr with hand-based physical proxy</div> <div class="author"> Ying Jiang, Congyi Zhang, Hongbo Fu, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Alberto Cannavo, Fabrizio Lamberti, Henry Y K Lau, Wenping Wang' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 32 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>3D sketching in virtual reality (VR) enables users to create 3D virtual objects intuitively and immersively. However, previous studies showed that mid-air drawing may lead to inaccurate sketches. To address this issue, we propose to use one hand as a canvas proxy and the index fnger of the other hand as a 3D pen. To this end, we frst perform a formative study to compare two-handed interaction with tablet-pen interaction for VR sketching. Based on the fndings of this study, we design HandPainter, a VR sketching system which focuses on the direct use of two hands for 3D sketching without requesting any tablet, pen, or VR controller. Our implementation is based on a pair of VR gloves, which provide hand tracking and gesture capture. We devise a set of intuitive gestures to control various functionalities required during 3D sketching, such as canvas panning and drawing positioning. We show the efectiveness of HandPainter by presenting a number of sketching results and discussing the outcomes of a user study-based comparison with mid-air drawing and tablet-based sketching tools. © 2021 ACM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Calandra2021380" class="col-sm-8"> <div class="title">Evaluating an Augmented Reality-Based Partially Assisted Approach to Remote Assistance in Heterogeneous Robotic Applications</div> <div class="author"> Davide Calandra, Alberto Cannavo, and Fabrizio Lamberti </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 2 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Among the countless applications of Augmented Reality (AR) in the industry, remote assistance represents one of the most prominent and widely studied use cases. Recently, the way in which assistance can be delivered started to evolve, unleashing the full potential of such technology. New methodologies have been proposed able to foster operators’ autonomy and reduce under-utilization of skilled human resources. This paper studies the effectiveness of a recently proposed approach to AR-based remote assistance, referred to as partially assisted, which differs from the traditional step-by-step guidance in the way the AR hints are conveyed by the expert to the operator. The suitability of this approach has been proved already for a number of simple industrial tasks, but a comprehensive study has yet to be performed for validating its effectiveness in complex use cases. This paper addresses this lack by considering as a case study the mastering of a robotic manipulator, a procedure involving a number of heterogeneous operations. The performance of the partially assisted approach is compared with step-by-step guidance based on both objective and subjective metrics. Results showed that the former approach could be particularly effective in reducing the time investment for the expert, allowing the operator to autonomously complete the assigned task in a time comparable to traditional assistance with a negligible need for further support. © 2021 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Pratticò2021" class="col-sm-8"> <div class="title">Exploring simulation-based virtual reality as a mock-up tool to support the design of first responders training</div> <div class="author"> Filippo Gabriele Pratticò, Federico De Lorenzis, Davide Calandra, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Alberto Cannavò, Fabrizio Lamberti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 7; All Open Access, Gold Open Access, Green Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Intervention by First Responders (FRs) is essential in disaster response, and their preparation greatly benefits from continuous updates. However, the design of effective training experiences targeted to FRs can be very demanding from the viewpoint of a Training Provisioner (TP). Virtual Reality (VR) may have a key role to play in enhancing and facilitating this task. In fact, VR technology has already proven to be very helpful in the field of emergency training, as well as its use as a powerful design and mock-up tool in many other contexts. In this work, the application of VR as a mock-up tool supporting TPs in the arrangement and validation of a training experience, either real or virtual, is explored. In particular, a case study is considered concerning the training of an FR for hydro-geological risks. Within this context, the proposed approach is compared against dramaturgy prototyping, a method commonly used for the design of experiential courses. Results indicate that the adoption of a VR-based mock-up tool (VRMT) can provide TPs with good insights on the arrangement of the training and precious indications on how to actually map this information onto real-world exercises. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Prattico20211157" class="col-sm-8"> <div class="title">Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for Autonomous Vehicle-to-Pedestrian Communication</div> <div class="author"> F. Gabriele Prattico, Fabrizio Lamberti, Alberto Cannavo, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Lia Morra, Paolo Montuschi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 44; All Open Access, Green Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Providing pedestrians and other vulnerable road users with a clear indication about a fully autonomous vehicle status and intentions is crucial to make them coexist. In the last few years, a variety of external interfaces have been proposed, leveraging different paradigms and technologies including vehicle-mounted devices (like LED panels), short-range on-road projections, and road infrastructure interfaces (e.g., special asphalts with embedded displays). These designs were experimented in different settings, using mockups, specially prepared vehicles, or virtual environments, with heterogeneous evaluation metrics. Promising interfaces based on Augmented Reality (AR) have been proposed too, but their usability and effectiveness have not been tested yet. This paper aims to complement such body of literature by presenting a comparison of state-of-the-art interfaces and new designs under common conditions. To this aim, an immersive Virtual Reality-based simulation was developed, recreating a well-known scenario represented by pedestrians crossing in urban environments under non-regulated conditions. A user study was then performed to investigate the various dimensions of vehicle-to-pedestrian interaction leveraging objective and subjective metrics. Even though no interface clearly stood out over all the considered dimensions, one of the AR designs achieved state-of-the-art results in terms of safety and trust, at the cost of higher cognitive effort and lower intuitiveness compared to LED panels showing anthropomorphic features. Together with rankings on the various dimensions, indications about advantages and drawbacks of the various alternatives that emerged from this study could provide important information for next developments in the field. © 1967-2012 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavo20216" class="col-sm-8"> <div class="title">How Blockchain, Virtual Reality, and Augmented Reality are Converging, and Why</div> <div class="author"> Alberto Cannavo, and Fabrizio Lamberti </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 40 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Nowadays, breakthrough technologies, such as virtual reality (VR), augmented reality (AR), and Blockchain, have definitively attracted the attention of a huge number of investors worldwide. Although, at first glance, Blockchain (traditionally used for financial services) seems to have little to none to share with VR and AR (originally adopted for entertainment), in the last few years several use cases started to appear showing effective ways to integrate these technologies. In this article, an overview of opportunities investigated by current solutions combining VR, AR, and Blockchain will be discussed, showing how they allowed both companies and academic researchers cope with issues affecting traditional services and products in a rather heterogenous set of application domains. Opportunities that could foster the convergence of these technologies and boost them further are also discussed. © 2012 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò20211871" class="col-sm-8"> <div class="title">An Evaluation Testbed for Locomotion in Virtual Reality</div> <div class="author"> Alberto Cannavò, Davide Calandra, F. Gabriele Pratticò, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Valentina Gatteschi, Fabrizio Lamberti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 33; All Open Access, Green Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>A common operation performed in Virtual Reality (VR) environments is locomotion. Although real walking can represent a natural and intuitive way to manage displacements in such environments, its use is generally limited by the size of the area tracked by the VR system (typically, the size of a room) or requires expensive technologies to cover particularly extended settings. A number of approaches have been proposed to enable effective explorations in VR, each characterized by different hardware requirements and costs, and capable to provide different levels of usability and performance. However, the lack of a well-defined methodology for assessing and comparing available approaches makes it difficult to identify, among the various alternatives, the best solutions for selected application domains. To deal with this issue, this article introduces a novel evaluation testbed which, by building on the outcomes of many separate works reported in the literature, aims to support a comprehensive analysis of the considered design space. An experimental protocol for collecting objective and subjective measures is proposed, together with a scoring system able to rank locomotion approaches based on a weighted set of requirements. Testbed usage is illustrated in a use case requesting to select the technique to adopt in a given application scenario. © 2020 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2021291" class="col-sm-8"> <div class="title">Evaluating Consumer Interaction Interfaces for 3D Sketching in Virtual Reality</div> <div class="author"> Alberto Cannavò, Davide Calandra, Aidan Kehoe, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 10 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Since its introduction, 3D mid-air sketching in immersive Virtual Reality (VR) proved to be a very powerful tool for many creative applications. However, common VR sketching suites rely on the standard hand controllers bundled with home VR systems, which are non-optimal for this kind of tasks. To deal with this issue, some research works proposed to use dedicated pen-shaped interfaces tracked with external motion-capture systems. Regrettably, these solutions are generally rather expensive, cumbersome and unsuitable for many potential end-users. Hence, lots of challenges regarding interfaces for 3D sketching in VR still exist. In this paper, a newly proposed sketching-oriented input device (namely, a VR stylus) compatible with the tracking technology of a consumer-grade VR system is compared with a standard hand controller from the same system. In particular, the paper reports the results of a user study whose aim was to evaluate, in both objective and subjective terms, aspects like, among others, sketching accuracy, ease of use, efficiency, comfort, control and naturalness. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2021109" class="col-sm-8"> <div class="title">An Automatic 3D Scene Generation Pipeline Based on a Single 2D Image</div> <div class="author"> Alberto Cannavò, Christian Bardella, Lorenzo Semeraro, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Federico De Lorenzis, Congyi Zhang, Ying Jiang, Fabrizio Lamberti' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> Cited by: 0 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In the last years, solutions were proposed in the literature to alleviate the complexity of using sophisticated graphic suites for 3D scene generation by leveraging automatic tools. The most common approach based on the processing of text descriptions, however, may not represent the ideal solution, e.g., for fast prototyping purposes. This paper proposes an alternative methodology able to extract information about the objects and the layout of the scene to be created from a single 2D image. Compared to previous works, experimental results reported in this work show improvements in terms of similarity between the 2D and 3D scenes. © 2021, Springer Nature Switzerland AG.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2020113" class="col-sm-8"> <div class="title">Automatic generation of affective 3D virtual environments from 2D images</div> <div class="author"> Alberto Cannavò, Arianna D’Alessandro, Daniele Maglione, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Giorgia Marullo, Congyi Zhang, Fabrizio Lamberti' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> Cited by: 8 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Today, a wide range of domains encompassing, e.g., movie and video game production, virtual reality simulations, augmented reality applications, make a massive use of 3D computer generated assets. Although many graphics suites already offer a large set of tools and functionalities to manage the creation of such contents, they are usually characterized by a steep learning curve. This aspect could make it difficult for non-expert users to create 3D scenes for, e.g., sharing their ideas or for prototyping purposes. This paper presents a computer-based system that is able to generate a possible reconstruction of a 3D scene depicted in a 2D image, by inferring objects, materials, textures, lights, and camera required for rendering. The integration of the proposed system into a well-known graphics suite enables further refinements of the generated scene using traditional techniques. Moreover, the system allows the users to explore the scene into an immersive virtual environment for better understanding the current objects’ layout, and provides the possibility to convey emotions through specific aspects of the generated scene. The paper also reports the results of a user study that was carried out to evaluate the usability of the proposed system from different perspectives. Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Botto2020299" class="col-sm-8"> <div class="title">Augmented Reality for the Manufacturing Industry: The Case of an Assembly Assistant</div> <div class="author"> Carola Botto, Alberto Cannavo, Daniele Cappuccio, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Giada Morat, Amir Nematollahi Sarvestani, Paolo Ricci, Valentina Demarchi, Alessandra Saturnino' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> Cited by: 21 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper studies the impact of augmented reality (AR) on manual assembly operations in the manufacturing industry. A review of ARbased solutions in this field shows that assembly assistants capable to support the user in key activities (identification, handling, alignment, joining, adjustment and inspection) are particularly beneficial. Since assembly activities are generally not addressed within comprehensive solutions, an AR-based tool supporting all of them is proposed, and its effectiveness in terms of completion time and error rate is compared with the use of the corresponding paper-based instructions. Results of a user study unveil that the proposed tool generally reduces the number of errors, but the time needed to complete the assembly tends to increase. Limitations of the current solution and potential directions for future work are discussed. © 2020 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò202051" class="col-sm-8"> <div class="title">Posing 3D Characters in Virtual Reality Through In-the-Air Sketches</div> <div class="author"> Alberto Cannavò, Congyi Zhang, Wenping Wang, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> Cited by: 6 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Generating computer animations is a very labor-intensive task, which requires animators to operate with sophisticated interfaces. Hence, researchers continuously experiment with alternative interaction paradigms that could possibly ease the above task. Among others, sketching represents a valid alternative to traditional interfaces since it can make interactions more expressive and intuitive; however, although the literature proposes several solutions leveraging sketch-based interfaces to solve different computer graphics challenges, generally they are not fully integrated in the computer animation pipeline. At the same time, Virtual Reality (VR), is becoming commonplace in many domains, and recently started to be recognized as capable to make it easier also the animators’ job, by improving their spatial understanding of the animated scene and providing them with interfaces characterized by higher usability and effectiveness. Based on all of the above, this paper presents an add-on for a well-known animation suite that combines the advantages offered by a sketch-based interface and VR to let animators define poses and create virtual character animations in an immersive environment. © 2020, Springer Nature Switzerland AG.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2020" class="col-sm-8"> <div class="title">A visual editing tool supporting the production of 3D interactive graphics assets for public exhibitions</div> <div class="author"> Alberto Cannavò, Francesco De Pace, Federico Salaroglio, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> Cited by: 11 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The introduction of interactive assets in public exhibitions is capable to significantly enhance the visitors’ user experience. However, the creation of interactive applications could represent a challenging task, especially for users lacking computer skills. Visual programming languages (VPLs) – one of the instruments belonging to the broad categories of methods and tools devised to support end-user development (EUD) – promise to offer an intuitive way to overcome these limitations, by providing easy-to-use and efficient interfaces for encoding applications’ logic. Moving from these considerations, this paper first analyses pros and cons of tools devised so far to support the generation of interactive contents. Then, it presents the design of a new tool named Visual Scene Editor (VSE), which allows users with little to no programming skills to create 3D interactive applications by combining available assets through an interactive, visual process. Both objective and subjective measurements have been collected with both skilled and unskilled users to evaluate the performance of the proposed tool. A comparison with existing solutions shows a reduction in the time required to complete the assigned tasks, of the complexity of the logic created, as well as of the number of errors made, confirming the suitability of the VSE for the said purpose. © 2020 Elsevier Ltd</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lamberti202036" class="col-sm-8"> <div class="title">Is Immersive Virtual Reality the Ultimate Interface for 3D Animators?</div> <div class="author"> Fabrizio Lamberti, Alberto Cannavo, and Paolo Montuschi </div> <div class="periodical"> 2020 </div> <div class="periodical"> Cited by: 20 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Creating computer animations is a labor-intensive task. Existing virtual reality (VR)-based animation solutions offer only heterogeneous subsets of traditional tools’ functionalities. We present an add-on for the Blender animation suite that enables users to switch between native and immersive VR-based interfaces and employ the latter to perform a representative set of tasks. © 1970-2012 IEEE.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavo2019125463" class="col-sm-8"> <div class="title">Immersive Virtual Reality-Based Interfaces for Character Animation</div> <div class="author"> Alberto Cannavo, Claudio Demartini, Lia Morra, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2019 </div> <div class="periodical"> Cited by: 40; All Open Access, Gold Open Access, Green Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Virtual Reality (VR) has increasingly attracted the attention of the computer animation community in search of more intuitive and effective alternatives to the current sophisticated user interfaces. Previous works in the literature already demonstrated the higher affordances offered by VR interaction, as well as the enhanced spatial understanding that arises thanks to the strong sense of immersion guaranteed by virtual environments. These factors have the potential to improve the animators’ job, which is tremendously skill-intensive and time-consuming. The present paper explores the opportunities provided by VR-based interfaces for the generation of 3D animations via armature deformation. To the best of the authors’ knowledge, for the first time a tool is presented which allows users to manage a complete pipeline supporting the above animation method, by letting them execute key tasks such as rigging, skinning and posing within a well-known animation suite using a customizable interface. Moreover, it is the first work to validate, in both objective and subjective terms, character animation performance in the above tasks and under realistic work conditions involving different user categories. In our experiments, task completion time was reduced by 26%, on average, while maintaining almost the same levels of accuracy and precision for both novice and experienced users. © 2013 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Prattico2019205" class="col-sm-8"> <div class="title">Investigating tangible user interaction in mixed-reality robotic games</div> <div class="author"> F. Gabriele Prattico, Piero Baldo, Alberto Cannavo, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2019 </div> <div class="periodical"> Cited by: 4 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Among the emerging trends in Human-Robot Interaction, some of the most frequently used paradigms of interaction involve the use of Tangible User Interfaces. This is especially true also in the field of robotic gaming and, more specifically, in application domains in which commercial off-the-shelf robots and projected Mixed Reality (MR) technology are combined together. The popularity of such interfaces, also in other domains of Human-Machine Interaction, has led to an abundance in the number of gestures that can be used to perform tangible action using these interfaces. However, there are not sufficient pieces of evidence on how these different modalities can impact the user experience, in particular when interacting with a robot in a "phygital play" environment. By moving from this consideration, this paper reports on the efforts that are ongoing with the aim to investigate the impact of diverse gesture sets (which can be performed with the same physical prop) on the perception of interaction with the robotic system. It also presents preliminary insights obtained, which could be exploited to orient further research about the use of such interfaces for interaction in MRbased robotic gaming and related scenarios. © 2019 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Pratticò201976" class="col-sm-8"> <div class="title">User Perception of Robot’s Role in Floor Projection-based Mixed-Reality Robotic Games</div> <div class="author"> F. Gabriele Pratticò, Alberto Cannavò, Junchao Chen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2019 </div> <div class="periodical"> Cited by: 4 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Within the emerging research area represented by robotic gaming and, specifically, in application domains in which the recent literature suggests to combine commercial off-the-shelf (COTS) robots and projected mixed reality (MR) technology in order to develop engaging games, one of the crucial issues to consider in the design process is how to make the player perceive the robot as having a key role, i.e., to valorize its presence from the user experience point of view. By moving from this consideration, this paper reports efforts that are being carried out with the aim to investigate the impact of diverse game design choices in the above perspective, while at the same time extracting preliminary insights that can be exploited to orient further research in the field of MR-based robotic gaming and related scenarios. © 2019 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lamberti2019" class="col-sm-8"> <div class="title">Designing Interactive Robotic Games based on Mixed Reality Technology</div> <div class="author"> Fabrizio Lamberti, Alberto Cannavo, and Paolo Pirone </div> <div class="periodical"> 2019 </div> <div class="periodical"> Cited by: 5 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper focuses on an emerging research area represented by robotic gaming and aims to explore the design space of interactive games that combine commercial-off-the-shelf robots and mixed reality. To this purpose, a software platform is developed which allows players to interact with both physical elements and virtual content projected on the ground. A game is then created to show designers how to maximize opportunities offered by such a technology and to build playful experiences. © 2019 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2019310" class="col-sm-8"> <div class="title">Automatic recognition of sport events from spatio-temporal data: An application for virtual reality-based training in basketball</div> <div class="author"> Alberto Cannavò, Davide Calandra, Gianpaolo Basilicò, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Fabrizio Lamberti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2019 </div> <div class="periodical"> Cited by: 4; All Open Access, Hybrid Gold Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Data analysis in the field of sport is growing rapidly due to the availability of datasets containing spatio-temporal positional data of the players and other sport equipment collected during the game. This paper investigates the use of machine learning for the automatic recognition of small-scale sport events in a basketball-related dataset. The results of the method discussed in this paper have been exploited to extend the functionality of an existing Virtual Reality (VR)-based tool supporting training in basketball. The tool allows the coaches to draw game tactics on a touchscreen, which can be then visualized and studies in an immersive VR environment by multiple players. Events recognized by the proposed system can be used to let the tool manage also previous matches, which can be automatically recreated by activating different animations for the virtual players and the ball based on the particular game situation, thus increasing the realism of the simulation. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lamberti20191655" class="col-sm-8"> <div class="title">A Multimodal Interface for Virtual Character Animation Based on Live Performance and Natural Language Processing</div> <div class="author"> Fabrizio Lamberti, Valentina Gatteschi, Andrea Sanna, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alberto Cannavò' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2019 </div> <div class="periodical"> Cited by: 8 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Virtual character animation is receiving an ever-growing attention by researchers, who proposed already many tools with the aim to improve the effectiveness of the production process. In particular, significant efforts are devoted to create animation systems suited also to non-skilled users, in order to let them benefit from a powerful communication instrument that can improve information sharing in many contexts like product design, education, marketing, etc. Apart from methods based on the traditional Windows-Icons-Menus-Pointer (WIMP) paradigms, solutions devised so far leverage approaches based on motion capture/retargeting (the so-called performance-based approaches), on non-conventional interfaces (voice inputs, sketches, tangible props, etc.), or on natural language processing (NLP) over text descriptions (e.g., to automatically trigger actions from a library). Each approach has its drawbacks, though. Performance-based methods are difficult to use for creating non-ordinary movements (flips, handstands, etc.); natural interfaces are often used for rough posing, but results need to be later refined; automatic techniques still produce poorly realistic animations. To deal with the above limitations, we propose a multimodal animation system that combines performance- and NLP-based methods. The system recognizes natural commands (gestures, voice inputs) issued by the performer, extracts scene data from a text description and creates live animations in which pre-recorded character actions can be blended with performer’s motion to increase naturalness. © 2018, © 2018 Taylor &amp; Francis Group, LLC.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Calandra20191403" class="col-sm-8"> <div class="title">Building reconfigurable passive haptic interfaces on demand using off-the-shelf construction bricks</div> <div class="author"> Davide Calandra, F. Gabriele Prattico, Alberto Cannavo, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Luca Micelli, Fabrizio Lamberti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2019 </div> <div class="periodical"> Cited by: 5; All Open Access, Bronze Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Although passive haptic interfaces have been shown to be capable to enhance the user’s sense of presence in Mixed Reality experiences, their use is still constrained by the need to rely on exact replicas of virtual objects or on custom-made devices mimicking the original ones. Unfortunately, the former are not flexible enough in terms of reconfigurability, whereas the latter may be difficult to reproduce. To tackle these issues, this paper explores the possibility to build passive haptic interfaces using off-the-shelf toy construction bricks. Bricks can be assembled to provide the intended feedback in more than one task. Moreover, they may be reassembled in another application to mimic completely new objects and support totally different tasks. © 2019 IEEE.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lamberti20181742" class="col-sm-8"> <div class="title">Virtual Character Animation Based on Affordable Motion Capture and Reconfigurable Tangible Interfaces</div> <div class="author"> Fabrizio Lamberti, Gianluca Paravati, Valentina Gatteschi, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Alberto Cannavo, Paolo Montuschi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2018 </div> <div class="periodical"> Cited by: 32; All Open Access, Green Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Software for computer animation is generally characterized by a steep learning curve, due to the entanglement of both sophisticated techniques and interaction methods required to control 3D geometries. This paper proposes a tool designed to support computer animation production processes by leveraging the affordances offered by articulated tangible user interfaces and motion capture retargeting solutions. To this aim, orientations of an instrumented prop are recorded together with animator’s motion in the 3D space and used to quickly pose characters in the virtual environment. High-level functionalities of the animation software are made accessible via a speech interface, thus letting the user control the animation pipeline via voice commands while focusing on his or her hands and body motion. The proposed solution exploits both off-the-shelf hardware components (like the Lego Mindstorms EV3 bricks and the Microsoft Kinect, used for building the tangible device and tracking animator’s skeleton) and free open-source software (like the Blender animation tool), thus representing an interesting solution also for beginners approaching the world of digital animation for the first time. Experimental results in different usage scenarios show the benefits offered by the designed interaction strategy with respect to a mouse &amp; keyboard-based interface both for expert and non-expert users. © 1995-2012 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò201826" class="col-sm-8"> <div class="title">A movement analysis system based on immersive virtual reality and wearable technology for sport training</div> <div class="author"> Alberto Cannavò, Giuseppe Ministeri, Fabrizio Lamberti, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Filippo Gabriele Pratticò' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2018 </div> <div class="periodical"> Cited by: 30 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The use of virtual reality (VR) is widespread in a growing number of application domains. Continuous technological advancements in the field of computer graphics made VR an interesting tool for learning purposes, especially in sport. Examples can be found in different sports such as rugby, baseball, soccer, golf, etc. This paper presents a VR-based training system that can be used as a self-learning tool to improve the execution of a given technical gesture. In particular, the basketball free throw gesture is considered. To assess the usefulness of the proposed system, experimental tests were carried out in a small-scale setup by involving 18 non-skilled volunteers. Results demonstrated that the designed system can improve the execution of the considered gesture in terms of both timing and spatial positioning compared to an alternative technique based on video projection. © 2018 Association for Computing Machinery.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lamberti20181017" class="col-sm-8"> <div class="title">Mobile Robot-based Exergames for Navigation Training and Vestibular Rehabilitation</div> <div class="author"> Fabrizio Lamberti, Alberto Cannavo, Paolo Pirone, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Carla Montuschi, Roberto Albera' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2018 </div> <div class="periodical"> Cited by: 1 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The vestibular system is the leading sensory system that contributes to the sense of balance and to spatial orientation for the purpose of movement coordination. Vestibular disorders are incredibly common, and exhibit many different symptoms including vertigo, unsteadiness and navigation issues, but also emotional and social problems. Many of the assessment, training and rehabilitation approaches developed so far cannot guarantee the necessary degree of usability, measurability and repeatability. This paper presents the preparatory steps towards the design of a methodology for treating vestibular disorders that combines established methods with innovative, robot-based exergames to foster, among others, engagement and flexibility. Preliminary results obtained through a user study that involved non-pathological subjects offered helpful indications that could be exploited in the design and validation of novel rehabilitation protocols in the field. © 2018 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò201845" class="col-sm-8"> <div class="title">A virtual character posing system based on reconfigurable tangible user interfaces and immersive virtual reality</div> <div class="author"> Alberto Cannavò, and Fabrizio Lamberti </div> <div class="periodical"> 2018 </div> <div class="periodical"> Cited by: 8 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Computer animation and, particularly, virtual character animation, are very time consuming and skill-intensive tasks, which require animators to work with sophisticated user interfaces. Tangible user interfaces (TUIs) already proved to be capable of making character animation more intuitive, and possibly more efficient, by leveraging the affordances provided by physical props that mimic the structure of virtual counterparts. The main downside of existing TUI-based animation solutions is the reduced accuracy, which is due partly to the use of mechanical parts, partly to the fact that, despite the adoption of a 3D input, users still have to work with a 2D output (usually represented by one or more views displayed on a screen). However, output methods that are natively 3D, e.g., based on virtual reality (VR), have been already exploited in different ways within computer animation scenarios. By moving from the above considerations and by building upon an existing work, this paper proposes a VR-based character animation system that combines the advantages of TUIs with the improved spatial awareness, enhanced visualization and better control on the observation point in the virtual space ensured by immersive VR. Results of a user study with both skilled and unskilled users showed a marked preference for the devised system, which was judged as more intuitive than that in the reference work, and allowed users to pose a virtual character in a lower time and with a higher accuracy. © 2018 The Eurographics Association.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cancedda2017447" class="col-sm-8"> <div class="title">Mixed reality-based user interaction feedback for a hand-controlled interface targeted to robot teleoperation</div> <div class="author"> Laura Cancedda, Alberto Cannavò, Giuseppe Garofalo, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Fabrizio Lamberti, Paolo Montuschi, Gianluca Paravati' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2017 </div> <div class="periodical"> Cited by: 9 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The continuous progress in the field of robotics and the diffusion of its related application scenarios in today’s modern world makes human interaction and communication with robots an aspect of fundamental importance. The development of interfaces based on natural interaction paradigms is getting an increasingly captivating topic in Human-Robot Interaction (HRI), due to their intrinsic capabilities in providing ever more intuitive and effective control modalities. Teleoperation systems require to handle a non-negligible amount of information coming from on-board sensors as well as input devices, thus increasing the workload of remote users. This paper presents the design of a 3D User Interface (3DUI) for the control of teleoperated robotic platforms aimed at increasing the interaction efficiency. A hand gesture driven controller is used as input modality to naturally map the position and gestures of the user’s hand to suitable commands for controlling the platform components. The designed interface leverages on mixed reality to provide a visual feedback to the control commands issued by the user. The visualization of the 3DUI is superimposed to the video stream provided by an on-board camera. A user study confirmed that the proposed solution is able to improve the interaction efficiency by significantly reducing the completion time for tasks assigned in a remote reach-and-pick scenario. © Springer International Publishing AG 2017.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò2017266" class="col-sm-8"> <div class="title">T4T: Tangible interface for tuning 3D object manipulation tools</div> <div class="author"> Alberto Cannavò, Fabio Cermelli, Vincenzo Chiaramida, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Giovanni Ciccone, Fabrizio Lamberti, Paolo Montuschi, Gianluca Paravati' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> 2017 </div> <div class="periodical"> Cited by: 3 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>A 3D User Interface for manipulating virtual objects in Augmented Reality scenarios on handheld devices is presented. The proposed solution takes advantage of two interaction techniques. The former (named ’cursor mode’) exploits a cursor, which position and movement are bound to the view of the device; the cursor allows the user to select objects and to perform coarse-grain manipulations by moving the device. The latter (referred to as ’tuning mode’) uses the physical affordances of a tangible interface to provide the user with the possibility to refine objects in all their aspects (position, rotation, scale, color, and so forth) with a fine-grained control. © 2017 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Attanasio2017256" class="col-sm-8"> <div class="title">HOT: Hold your own tools for AR-based constructive art</div> <div class="author"> Giuseppe Attanasio, Alberto Cannavò, Francesca Cibrario, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Fabrizio Lamberti, Paolo Montuschi, Gianluca Paravati' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2017 </div> <div class="periodical"> Cited by: 0 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Using digital instruments to support artistic expression and creativity is a hot topic. In this work, we focused on the design of a suitable interface for Augmented Reality-based constructive art on handheld devices. Issues to be faced encompassed how to give artists sense of spatial dimensions, how to provide them with different tools for realizing artworks, and how much moving away from ’the real’ and going towards ’the virtual’. Through a touch-capable device, such as a smartphone or a tablet, we offer artists a clean workspace, where they can decide when to introduce artworks and tools. In fact, besides exploiting the multi-touch functionality and the gyroscopes/accelerometers to manipulate artworks in six degrees of freedom (6DOF), the proposed solution exploits a set of printed markers that can be brought into the camera’s field of view to make specific virtual tools appear in the augmented scene. With such tools, artists can decide to control, e.g., manipulation speed, scale factor, scene parameters, etc., thus complementing functionalities that can be accessed via the device’s screen. © 2017 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lamberti20171989" class="col-sm-8"> <div class="title">Supporting web analytics by aggregating user interaction data from heterogeneous devices using viewport-DOM-based heat maps</div> <div class="author"> Fabrizio Lamberti, Gianluca Paravati, Valentina Gatteschi, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alberto Cannavo' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2017 </div> <div class="periodical"> Cited by: 19; All Open Access, Green Open Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The players of the digital industry look at network Big Data as an incredible source of revenues, which can allow them to design products, services, and market strategies ever more tailored to users’ interests and needs. This is the case of data collected by Web analytics tools, which describe the way users interact with Web contents and where their attention focuses onto during navigation. Given the complexity of information to analyze, existing tools often make use of visualization strategies to represent data aggregated throughout separate sessions and multiple users. In particular, heat maps are often adopted to study the distribution of mouse activity and identify page regions that are more frequently reached during interaction. Unfortunately, since Web contents are accessed via ever more heterogeneous devices, region-based heat maps cannot be exploited anymore to aggregate data concerning user’s attention, since the same Web content may move to another page location or exhibit a different aspect depending on the access device used or the user agent setup. This paper presents the design of a visual analytics framework capable to deal with the above limitation by adopting a data collection approach that combines information about regions displayed with information about page structure. This way, the well-known heat map-based visualization can be produced, where interactions can be aggregated on a per-element basis independently of the specific access configuration. Experimental results showed that the framework succeeds in accurately quantifying user’s attention and replicating results obtained by manual processing. © 2005-2012 IEEE.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cannavò201753" class="col-sm-8"> <div class="title">User interaction feedback in a hand-controlled interface for robot team tele-operation using wearable augmented reality</div> <div class="author"> Alberto Cannavò, and Fabrizio Lamberti </div> <div class="periodical"> 2017 </div> <div class="periodical"> Cited by: 0 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Continuous advancements in the field of robotics and its increasing spread across heterogeneous application scenarios make the development of ever more effective user interfaces for human-robot interaction (HRI) an extremely relevant research topic. In particular, Natural User Interfaces (NUIs), e.g., based on hand and body gestures, proved to be an interesting technology to be exploited for designing intuitive interaction paradigms in the field of HRI. However, the more sophisticated the HRI interfaces become, the more important is to provide users with an accurate feedback about the state of the robot as well as of the interface itself. In this work, an Augmented Reality (AR)-based interface is deployed on a head-mounted display to enable tele-operation of a remote robot team using hand movements and gestures. A user study is performed to assess the advantages of wearable AR compared to desktop-based AR in the execution of specific tasks. © 2017 The Author(s) Eurographics Proceedings © 2017 The Eurographics Association.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Bonaiuto2017555" class="col-sm-8"> <div class="title">Tele-operation of Robot Teams: A Comparison of Gamepad-, Mobile Device and Hand Tracking-Based User Interfaces</div> <div class="author"> Stefano Bonaiuto, Alberto Cannavò, Giovanni Piumatti, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Gianluca Paravati, Fabrizio Lamberti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2017 </div> <div class="periodical"> Cited by: 11 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Due to the continuous advancements made in robot technologies, the development of intuitive and effective user interfaces for human-robot interaction is getting increasingly important. This paper investigates how different types of interfaces can be used for allowing a single operator to remotely control a team of robots endowed with different capabilities. Attention is focused on three user interfaces based on a gamepad, on a mobile device and on hand tracking, respectively. To evaluate pros and cons of the above interfaces, a user study was conducted, in which participants had to combine the capabilities of a rover, a drone and a robotic arm in order to carry out a search and pick task. Based on the experiments, the fastest way to complete the task was to use the mobile device. However, results showed that some of the interfaces could provide better performances for selected robots and associated sub-tasks. It is worth observing that, despite evidences about efficiency, participants rated the gamepad as the preferred interface from the point of view of subjective usability. © 2017 IEEE.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Alberto Cannavò. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a> Last updated: April 02, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:"Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6C%62%65%72%74%6F.%63%61%6E%6E%61%76%6F@%70%6F%6C%69%74%6F.%69%74","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-6884-9268","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=P-3NxFMAAAAJ&hl","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=57194155419","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/alberto-cannav%C3%B2-978a37b7/","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://www.polito.it/personale?p=039345","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/198/4198.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>